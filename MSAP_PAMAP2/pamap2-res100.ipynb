{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-11T06:21:00.751900Z","iopub.status.busy":"2024-02-11T06:21:00.751525Z","iopub.status.idle":"2024-02-11T06:21:01.116701Z","shell.execute_reply":"2024-02-11T06:21:01.115800Z","shell.execute_reply.started":"2024-02-11T06:21:00.751869Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T06:21:01.119651Z","iopub.status.busy":"2024-02-11T06:21:01.118847Z","iopub.status.idle":"2024-02-11T06:21:01.124444Z","shell.execute_reply":"2024-02-11T06:21:01.123048Z","shell.execute_reply.started":"2024-02-11T06:21:01.119616Z"},"trusted":true},"outputs":[],"source":["# x_train_path = \"/kaggle/input/wisdm-data/wisdm/x_train.npy\"\n","# y_train_path = \"/kaggle/input/wisdm-data/wisdm/y_train.npy\"\n","x_train_path = \"/root/HAR/dataset/PAMAP2/x_train.npy\"\n","y_train_path = \"/root/HAR/dataset/PAMAP2/y_train.npy\"\n","# x_test_path = \"/kaggle/input/wisdm-data/wisdm/x_test.npy\"\n","# y_test_path = \"/kaggle/input/wisdm-data/wisdm/y_test.npy\"\n","x_test_path = \"/root/HAR/dataset/PAMAP2/x_test.npy\"\n","y_test_path = \"/root/HAR/dataset/PAMAP2/y_test.npy\""]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T06:21:01.126163Z","iopub.status.busy":"2024-02-11T06:21:01.125884Z","iopub.status.idle":"2024-02-11T06:21:04.273964Z","shell.execute_reply":"2024-02-11T06:21:04.273176Z","shell.execute_reply.started":"2024-02-11T06:21:01.126139Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SEModule(nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super().__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n","        self.fc1 = nn.Conv1d(channels, channels // reduction, kernel_size=1, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv1d(channels // reduction, channels, kernel_size=1, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        y = self.avg_pool(x)\n","        y = self.fc1(y)\n","        y = self.relu(y)\n","        y = self.fc2(y)\n","        y = self.sigmoid(y)\n","        return x * y\n","\n","\n","class ChannelAttention1d(nn.Module):\n","\n","    def __init__(self, in_channels, ratio=16):\n","        super(ChannelAttention1d, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n","        self.max_pool = nn.AdaptiveMaxPool1d(1)\n","\n","        self.fc1 = nn.Conv1d(in_channels, in_channels//16, 1, bias=False)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Conv1d(in_channels//16, in_channels, 1, bias=False)\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n","        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n","        out = avg_out + max_out\n","        return self.sigmoid(out)\n","\n","class SpatialAttention1d(nn.Module):\n","\n","    def __init__(self, kernel_size=3):\n","        super(SpatialAttention1d, self).__init__()\n","        self.conv = nn.Conv1d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x = torch.cat([avg_out, max_out], dim=1)\n","        x = self.conv(x)\n","        return self.sigmoid(x)\n","\n","class rSoftMax(nn.Module):\n","    def __init__(self, radix, cardinality):\n","        super().__init__()\n","        self.radix = radix\n","        self.cardinality = cardinality\n","\n","    def forward(self, x):\n","        if self.radix > 1:\n","            batch = x.size(0)\n","            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n","            x = F.softmax(x, dim=1)\n","            x = x.reshape(batch, -1)\n","        else:\n","            x = torch.sigmoid(x)\n","        return x\n","\n","\n","class DropBlock1D(object):\n","    def __init__(self, *args, **kwargs):\n","        raise NotImplementedError\n","\n","\n","class SplAtConv1d(nn.Module):\n","    def __init__(\n","            self,\n","            in_channels,\n","            channels,\n","            kernel_size,\n","            stride=1,\n","            padding=0,\n","            dilation=1,\n","            groups=1,\n","            bias=True,\n","            radix=2,\n","            reduction_factor=4,\n","            norm_layer=None,\n","            dropblock_prob=0.0,\n","            **kwargs,\n","\n","    ):\n","        super().__init__()\n","\n","        self.dropblock_prob = dropblock_prob\n","        inter_channels = max(in_channels * radix // reduction_factor, 32)\n","        self.radix = radix\n","        self.cardinality = groups\n","        self.channels = channels\n","\n","        self.conv = nn.Conv1d(\n","            in_channels,\n","            channels * radix,\n","            kernel_size,\n","            stride,\n","            padding,\n","            dilation,\n","            groups=groups * radix,\n","            bias=bias,\n","            **kwargs,\n","        )\n","        self.bn0 = norm_layer(self.channels * radix)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc1 = nn.Conv1d(self.channels, inter_channels, 1, groups=self.cardinality)\n","        self.bn1 = norm_layer(inter_channels)\n","        self.fc2 = nn.Conv1d(inter_channels, self.channels * radix, 1, groups=self.cardinality)\n","        if dropblock_prob > 0.0:\n","            self.dropblock = DropBlock1D(dropblock_prob, 3)\n","        self.rsoftmax = rSoftMax(radix, self.cardinality)\n","        self.se = SEModule(in_channels)\n","        self.ca1 = ChannelAttention1d(in_channels)\n","        self.sa1 = SpatialAttention1d()\n","\n","    def forward(self, x):\n","        #x = self.ca1(x) * x\n","        #x=  self.sa1(x) * x\n","        #x = self.se(x) * x\n","        x = self.conv(x)\n","        x = self.bn0(x)\n","        if self.dropblock_prob > 0.0:\n","            x = self.dropblock(x)\n","        x = self.relu(x)\n","\n","        batch, rchannel = x.shape[:2]\n","        if self.radix > 1:\n","            splited = torch.split(x, int(rchannel // self.radix), dim=1)\n","            gap = sum(splited)\n","        else:\n","            gap = x\n","        #se放在这个位置结果非常好\n","        #位置1，通道注意力\n","        gap = self.ca1(gap) * gap\n","        #gap = self.sa1(gap) * gap\n","        #gap = self.se(gap) * gap\n","        gap = F.adaptive_avg_pool1d(gap, 1)\n","        gap = self.fc1(gap)\n","        gap = self.bn1(gap)\n","        gap = self.relu(gap)\n","\n","        atten = self.fc2(gap)\n","        atten = self.rsoftmax(atten).view(batch, -1, 1)\n","\n","        if self.radix > 1:\n","            attens = torch.split(atten, int(rchannel // self.radix), dim=1)\n","            outs = []\n","            for att, split in zip(attens, splited):\n","                outs.append(att * split)\n","            out = sum(outs)\n","        else:\n","            out = atten * x\n","       # out = self.se(out) * out\n","        #out = self.ca1(out) * out\n","        #out = self.sa1(out) * out\n","        return out.contiguous()\n","\n","\n","\n","\n","\n","class ResNeStBottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(\n","            self,\n","            inplanes,\n","            planes,\n","            stride=1,\n","            downsample=None,\n","            radix=1,\n","            cardinality=1,\n","            bottleneck_width=64,\n","            avd=False,\n","            avd_first=False,\n","            dilation=1,\n","            is_first=False,\n","            norm_layer=None,\n","            last_gamma=False,\n","            dropblock_prob=0.0\n","    ):\n","        super().__init__()\n","        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n","\n","        self.conv1 = nn.Conv1d(inplanes, group_width, kernel_size=1, bias=False)\n","        self.bn1 = norm_layer(group_width)\n","        self.radix = radix\n","        self.avd = avd and (stride > 1 or is_first)\n","        self.avd_first = avd_first\n","        self.dropblock_prob = dropblock_prob\n","\n","        if self.avd:\n","            self.avd_layer = nn.AvgPool1d(3, stride, padding=1)\n","            stride = 1\n","        if dropblock_prob > 0.0:\n","            self.dropblock1 = DropBlock1D(dropblock_prob, 3)\n","            if radix == 1:\n","                self.dropblock2 = DropBlock1D(dropblock_prob, 3)\n","            self.dropblock3 = DropBlock1D(dropblock_prob, 3)\n","\n","        if radix >= 1:\n","            self.conv2 = SplAtConv1d(\n","                group_width,\n","                group_width,\n","                kernel_size=3,\n","                stride=stride,\n","                padding=dilation,\n","                dilation=dilation,\n","                groups=cardinality,\n","                bias=False,\n","                radix=radix,\n","                norm_layer=norm_layer,\n","                dropblock_prob=dropblock_prob\n","            )\n","        else:\n","            self.conv2 = nn.Conv1d(\n","                group_width,\n","                group_width,\n","                kernel_size=3,\n","                stride=stride,\n","                padding=dilation,\n","                dilation=dilation,\n","                groups=cardinality,\n","                bias=False,\n","            )\n","            self.bn2 = norm_layer(group_width)\n","\n","        self.conv3 = nn.Conv1d(group_width, planes * self.expansion, kernel_size=1, bias=False)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","\n","        if last_gamma:\n","            from torch.nn.init import zeros_\n","\n","            zeros_(self.bn3.weight)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.dilation = dilation\n","        self.stride = stride\n","        self.ca1 = ChannelAttention1d(group_width)\n","        self.sa1 = SpatialAttention1d()\n","\n","        self.se = SEModule(group_width)\n","        \n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        #x = self.ca1(x) * x\n","        #位置2，空间注意力\n","       \n","        out = self.sa1(out) * out\n","        out = self.bn1(out)\n","        if self.dropblock_prob > 0.0:\n","            out = self.dropblock1(out)\n","        out = self.relu(out)\n","        if self.avd and self.avd_first:\n","            out = self.avd_layer(out)\n","        #out = self.ca1(out) * out\n","        #out=  self.sa1(out) * out\n","        #out = self.se(out) * out\n","        out = self.conv2(out)\n","        if self.radix == 0:\n","            out = self.bn2(out)\n","            if self.dropblock_prob > 0.0:\n","                out = self.dropblock2(out)\n","            out = self.relu(out)\n","\n","        if self.avd and not self.avd_first:\n","            out = self.avd_layer(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        if self.dropblock_prob > 0.0:\n","            out = self.dropblock3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(residual)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T06:21:04.275592Z","iopub.status.busy":"2024-02-11T06:21:04.275183Z","iopub.status.idle":"2024-02-11T06:21:04.334146Z","shell.execute_reply":"2024-02-11T06:21:04.333193Z","shell.execute_reply.started":"2024-02-11T06:21:04.275567Z"},"trusted":true},"outputs":[],"source":["class SplAtConv1d_WithoutCA(nn.Module):\n","    def __init__(\n","            self,\n","            in_channels,\n","            channels,\n","            kernel_size,\n","            stride=1,\n","            padding=0,\n","            dilation=1,\n","            groups=1,\n","            bias=True,\n","            radix=2,\n","            reduction_factor=4,\n","            norm_layer=None,\n","            dropblock_prob=0.0,\n","            **kwargs,\n","\n","    ):\n","        super().__init__()\n","\n","        self.dropblock_prob = dropblock_prob\n","        inter_channels = max(in_channels * radix // reduction_factor, 32)\n","        self.radix = radix\n","        self.cardinality = groups\n","        self.channels = channels\n","\n","        self.conv = nn.Conv1d(\n","            in_channels,\n","            channels * radix,\n","            kernel_size,\n","            stride,\n","            padding,\n","            dilation,\n","            groups=groups * radix,\n","            bias=bias,\n","            **kwargs,\n","        )\n","        self.bn0 = norm_layer(self.channels * radix)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc1 = nn.Conv1d(self.channels, inter_channels, 1, groups=self.cardinality)\n","        self.bn1 = norm_layer(inter_channels)\n","        self.fc2 = nn.Conv1d(inter_channels, self.channels * radix, 1, groups=self.cardinality)\n","        if dropblock_prob > 0.0:\n","            self.dropblock = DropBlock1D(dropblock_prob, 3)\n","        self.rsoftmax = rSoftMax(radix, self.cardinality)\n","        self.se = SEModule(in_channels)\n","        self.ca1 = ChannelAttention1d(in_channels)\n","        self.sa1 = SpatialAttention1d()\n","\n","    def forward(self, x):\n","        #x = self.ca1(x) * x\n","        #x=  self.sa1(x) * x\n","        #x = self.se(x) * x\n","        x = self.conv(x)\n","        x = self.bn0(x)\n","        if self.dropblock_prob > 0.0:\n","            x = self.dropblock(x)\n","        x = self.relu(x)\n","\n","        batch, rchannel = x.shape[:2]\n","        if self.radix > 1:\n","            splited = torch.split(x, int(rchannel // self.radix), dim=1)\n","            gap = sum(splited)\n","        else:\n","            gap = x\n","        #se放在这个位置结果非常好\n","        #位置1，通道注意力\n","        #gap = self.ca1(gap) * gap\n","        #gap = self.sa1(gap) * gap\n","        #gap = self.se(gap) * gap\n","        gap = F.adaptive_avg_pool1d(gap, 1)\n","        gap = self.fc1(gap)\n","        gap = self.bn1(gap)\n","        gap = self.relu(gap)\n","\n","        atten = self.fc2(gap)\n","        atten = self.rsoftmax(atten).view(batch, -1, 1)\n","\n","        if self.radix > 1:\n","            attens = torch.split(atten, int(rchannel // self.radix), dim=1)\n","            outs = []\n","            for att, split in zip(attens, splited):\n","                outs.append(att * split)\n","            out = sum(outs)\n","        else:\n","            out = atten * x\n","       # out = self.se(out) * out\n","        #out = self.ca1(out) * out\n","        #out = self.sa1(out) * out\n","        return out.contiguous()\n","    \n","class ResNeStBottleneck_WithoutCA(nn.Module):\n","    expansion = 4\n","\n","    def __init__(\n","            self,\n","            inplanes,\n","            planes,\n","            stride=1,\n","            downsample=None,\n","            radix=1,\n","            cardinality=1,\n","            bottleneck_width=64,\n","            avd=False,\n","            avd_first=False,\n","            dilation=1,\n","            is_first=False,\n","            norm_layer=None,\n","            last_gamma=False,\n","            dropblock_prob=0.0\n","    ):\n","        super().__init__()\n","        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n","\n","        self.conv1 = nn.Conv1d(inplanes, group_width, kernel_size=1, bias=False)\n","        self.bn1 = norm_layer(group_width)\n","        self.radix = radix\n","        self.avd = avd and (stride > 1 or is_first)\n","        self.avd_first = avd_first\n","        self.dropblock_prob = dropblock_prob\n","\n","        if self.avd:\n","            self.avd_layer = nn.AvgPool1d(3, stride, padding=1)\n","            stride = 1\n","        if dropblock_prob > 0.0:\n","            self.dropblock1 = DropBlock1D(dropblock_prob, 3)\n","            if radix == 1:\n","                self.dropblock2 = DropBlock1D(dropblock_prob, 3)\n","            self.dropblock3 = DropBlock1D(dropblock_prob, 3)\n","\n","        if radix >= 1:\n","            self.conv2 = SplAtConv1d_WithoutCA(\n","                group_width,\n","                group_width,\n","                kernel_size=3,\n","                stride=stride,\n","                padding=dilation,\n","                dilation=dilation,\n","                groups=cardinality,\n","                bias=False,\n","                radix=radix,\n","                norm_layer=norm_layer,\n","                dropblock_prob=dropblock_prob\n","            )\n","        else:\n","            self.conv2 = nn.Conv1d(\n","                group_width,\n","                group_width,\n","                kernel_size=3,\n","                stride=stride,\n","                padding=dilation,\n","                dilation=dilation,\n","                groups=cardinality,\n","                bias=False,\n","            )\n","            self.bn2 = norm_layer(group_width)\n","\n","        self.conv3 = nn.Conv1d(group_width, planes * self.expansion, kernel_size=1, bias=False)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","\n","        if last_gamma:\n","            from torch.nn.init import zeros_\n","\n","            zeros_(self.bn3.weight)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.dilation = dilation\n","        self.stride = stride\n","        self.ca1 = ChannelAttention1d(group_width)\n","        self.sa1 = SpatialAttention1d()\n","\n","        self.se = SEModule(group_width)\n","        \n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        #x = self.ca1(x) * x\n","        #位置2，空间注意力\n","        #x=  self.sa1(x) * x\n","        out = self.sa1(out) * out\n","        out = self.bn1(out)\n","        if self.dropblock_prob > 0.0:\n","            out = self.dropblock1(out)\n","        out = self.relu(out)\n","        if self.avd and self.avd_first:\n","            out = self.avd_layer(out)\n","        #out = self.ca1(out) * out\n","        #out=  self.sa1(out) * out\n","        #out = self.se(out) * out\n","        out = self.conv2(out)\n","        if self.radix == 0:\n","            out = self.bn2(out)\n","            if self.dropblock_prob > 0.0:\n","                out = self.dropblock2(out)\n","            out = self.relu(out)\n","\n","        if self.avd and not self.avd_first:\n","            out = self.avd_layer(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        if self.dropblock_prob > 0.0:\n","            out = self.dropblock3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(residual)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","class ResNeStBottleneck_WithoutSA(nn.Module):\n","    expansion = 4\n","\n","    def __init__(\n","            self,\n","            inplanes,\n","            planes,\n","            stride=1,\n","            downsample=None,\n","            radix=1,\n","            cardinality=1,\n","            bottleneck_width=64,\n","            avd=False,\n","            avd_first=False,\n","            dilation=1,\n","            is_first=False,\n","            norm_layer=None,\n","            last_gamma=False,\n","            dropblock_prob=0.0\n","    ):\n","        super().__init__()\n","        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n","\n","        self.conv1 = nn.Conv1d(inplanes, group_width, kernel_size=1, bias=False)\n","        self.bn1 = norm_layer(group_width)\n","        self.radix = radix\n","        self.avd = avd and (stride > 1 or is_first)\n","        self.avd_first = avd_first\n","        self.dropblock_prob = dropblock_prob\n","\n","        if self.avd:\n","            self.avd_layer = nn.AvgPool1d(3, stride, padding=1)\n","            stride = 1\n","        if dropblock_prob > 0.0:\n","            self.dropblock1 = DropBlock1D(dropblock_prob, 3)\n","            if radix == 1:\n","                self.dropblock2 = DropBlock1D(dropblock_prob, 3)\n","            self.dropblock3 = DropBlock1D(dropblock_prob, 3)\n","\n","        if radix >= 1:\n","            self.conv2 = SplAtConv1d(\n","                group_width,\n","                group_width,\n","                kernel_size=3,\n","                stride=stride,\n","                padding=dilation,\n","                dilation=dilation,\n","                groups=cardinality,\n","                bias=False,\n","                radix=radix,\n","                norm_layer=norm_layer,\n","                dropblock_prob=dropblock_prob\n","            )\n","        else:\n","            self.conv2 = nn.Conv1d(\n","                group_width,\n","                group_width,\n","                kernel_size=3,\n","                stride=stride,\n","                padding=dilation,\n","                dilation=dilation,\n","                groups=cardinality,\n","                bias=False,\n","            )\n","            self.bn2 = norm_layer(group_width)\n","\n","        self.conv3 = nn.Conv1d(group_width, planes * self.expansion, kernel_size=1, bias=False)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","\n","        if last_gamma:\n","            from torch.nn.init import zeros_\n","\n","            zeros_(self.bn3.weight)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.dilation = dilation\n","        self.stride = stride\n","        self.ca1 = ChannelAttention1d(group_width)\n","        self.sa1 = SpatialAttention1d()\n","\n","        self.se = SEModule(group_width)\n","        \n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        #x = self.ca1(x) * x\n","        #位置2，空间注意力\n","       \n","        #out = self.sa1(out) * out\n","        out = self.bn1(out)\n","        if self.dropblock_prob > 0.0:\n","            out = self.dropblock1(out)\n","        out = self.relu(out)\n","        if self.avd and self.avd_first:\n","            out = self.avd_layer(out)\n","        #out = self.ca1(out) * out\n","        #out=  self.sa1(out) * out\n","        #out = self.se(out) * out\n","        out = self.conv2(out)\n","        if self.radix == 0:\n","            out = self.bn2(out)\n","            if self.dropblock_prob > 0.0:\n","                out = self.dropblock2(out)\n","            out = self.relu(out)\n","\n","        if self.avd and not self.avd_first:\n","            out = self.avd_layer(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        if self.dropblock_prob > 0.0:\n","            out = self.dropblock3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(residual)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","    \n","class ResNeStBottleneck_WithoutCASA(nn.Module):\n","    expansion = 4\n","\n","    def __init__(\n","            self,\n","            inplanes,\n","            planes,\n","            stride=1,\n","            downsample=None,\n","            radix=1,\n","            cardinality=1,\n","            bottleneck_width=64,\n","            avd=False,\n","            avd_first=False,\n","            dilation=1,\n","            is_first=False,\n","            norm_layer=None,\n","            last_gamma=False,\n","            dropblock_prob=0.0\n","    ):\n","        super().__init__()\n","        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n","\n","        self.conv1 = nn.Conv1d(inplanes, group_width, kernel_size=1, bias=False)\n","        self.bn1 = norm_layer(group_width)\n","        self.radix = radix\n","        self.avd = avd and (stride > 1 or is_first)\n","        self.avd_first = avd_first\n","        self.dropblock_prob = dropblock_prob\n","\n","        if self.avd:\n","            self.avd_layer = nn.AvgPool1d(3, stride, padding=1)\n","            stride = 1\n","        if dropblock_prob > 0.0:\n","            self.dropblock1 = DropBlock1D(dropblock_prob, 3)\n","            if radix == 1:\n","                self.dropblock2 = DropBlock1D(dropblock_prob, 3)\n","            self.dropblock3 = DropBlock1D(dropblock_prob, 3)\n","\n","        if radix >= 1:\n","            self.conv2 = SplAtConv1d_WithoutCA(\n","                group_width,\n","                group_width,\n","                kernel_size=3,\n","                stride=stride,\n","                padding=dilation,\n","                dilation=dilation,\n","                groups=cardinality,\n","                bias=False,\n","                radix=radix,\n","                norm_layer=norm_layer,\n","                dropblock_prob=dropblock_prob\n","            )\n","        else:\n","            self.conv2 = nn.Conv1d(\n","                group_width,\n","                group_width,\n","                kernel_size=3,\n","                stride=stride,\n","                padding=dilation,\n","                dilation=dilation,\n","                groups=cardinality,\n","                bias=False,\n","            )\n","            self.bn2 = norm_layer(group_width)\n","\n","        self.conv3 = nn.Conv1d(group_width, planes * self.expansion, kernel_size=1, bias=False)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","\n","        if last_gamma:\n","            from torch.nn.init import zeros_\n","\n","            zeros_(self.bn3.weight)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.dilation = dilation\n","        self.stride = stride\n","        self.ca1 = ChannelAttention1d(group_width)\n","        self.sa1 = SpatialAttention1d()\n","\n","        self.se = SEModule(group_width)\n","        \n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        #x = self.ca1(x) * x\n","        #位置2，空间注意力\n","       \n","        #out = self.sa1(out) * out\n","        out = self.bn1(out)\n","        if self.dropblock_prob > 0.0:\n","            out = self.dropblock1(out)\n","        out = self.relu(out)\n","        if self.avd and self.avd_first:\n","            out = self.avd_layer(out)\n","        #out = self.ca1(out) * out\n","        #out=  self.sa1(out) * out\n","        #out = self.se(out) * out\n","        out = self.conv2(out)\n","        if self.radix == 0:\n","            out = self.bn2(out)\n","            if self.dropblock_prob > 0.0:\n","                out = self.dropblock2(out)\n","            out = self.relu(out)\n","\n","        if self.avd and not self.avd_first:\n","            out = self.avd_layer(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        if self.dropblock_prob > 0.0:\n","            out = self.dropblock3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(residual)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T06:21:04.337683Z","iopub.status.busy":"2024-02-11T06:21:04.337155Z","iopub.status.idle":"2024-02-11T06:21:04.368434Z","shell.execute_reply":"2024-02-11T06:21:04.367520Z","shell.execute_reply.started":"2024-02-11T06:21:04.337657Z"},"trusted":true},"outputs":[],"source":["class ResNeSt1d(nn.Module):\n","    def __init__(\n","            self,\n","            inchannels,\n","            block,\n","            layers,\n","            radix=1,\n","            groups=1,\n","            bottleneck_width=64,\n","            num_classes=1000,\n","            dilated=False,\n","            dilation=1,\n","            deep_stem=False,\n","            stem_width=32,\n","            avg_down=False,\n","            avd=False,\n","            avd_first=False,\n","            final_drop=0.0,\n","            last_gamma=False,\n","            norm_layer=nn.BatchNorm1d,\n","            dropblock_prob=0\n","    ):\n","        super().__init__()\n","\n","        self.cardinality = groups\n","        self.bottleneck_width = bottleneck_width\n","        # ResNet-D params\n","        self.inplanes = stem_width * 2 if deep_stem else 64\n","        self.avg_down = avg_down\n","        self.last_gamma = last_gamma\n","        # ResNeSt params\n","        self.radix = radix\n","        self.avd = avd\n","        self.avd_first = avd_first\n","        \n","        \n","        act = nn.ReLU\n","\n","        if deep_stem:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv1d(inchannels, stem_width, 3, 2, 1, bias=False),\n","                norm_layer(stem_width),\n","                act(inplace=True),\n","                nn.Conv1d(stem_width, stem_width, 3, 1, 1, bias=False),\n","                norm_layer(stem_width),\n","                act(inplace=True),\n","                nn.Conv1d(stem_width, self.inplanes, 3, 1, 1, bias=False),\n","            )\n","        else:\n","            self.conv1 = nn.Conv1d(inchannels, self.inplanes, 7, 2, 3, bias=False)\n","\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = act(inplace=True)\n","        self.maxpool = nn.MaxPool1d(3, 2, 1)\n","        b = 16\n","        Blist = [b * 2, b * 4, b * 8, b * 16]\n","        self.layer1 = self._make_layer(block, Blist[0], layers[0], norm_layer=norm_layer, is_first=False)\n","        self.layer2 = self._make_layer(block, Blist[1], layers[1], stride=2, norm_layer=norm_layer)\n","        if dilated or dilation == 4:\n","            self.layer3 = self._make_layer(block, Blist[2], layers[2], stride=1, dilation=2, norm_layer=norm_layer,\n","                                           dropblock_prob=dropblock_prob)\n","            # self.layer4 = self._make_layer(block, Blist[3], layers[2], stride=1, dilation=2, norm_layer=norm_layer,dropblock_prob=dropblock_prob)\n","        elif dilation == 2:\n","            self.layer3 = self._make_layer(block, Blist[2], layers[2], stride=2, dilation=1, norm_layer=norm_layer,\n","                                           dropblock_prob=dropblock_prob)\n","            # self.layer4 = self._make_layer(block, Blist[3], layers[2], stride=1, dilation=2, norm_layer=norm_layer,dropblock_prob=dropblock_prob)\n","        else:\n","            self.layer3 = self._make_layer(block, Blist[2], layers[2], stride=2, dilation=1, norm_layer=norm_layer,\n","                                           dropblock_prob=dropblock_prob)\n","            self.layer4 = self._make_layer(block, Blist[3], layers[2], stride=2, dilation=1, norm_layer=norm_layer,dropblock_prob=dropblock_prob)\n","\n","        self.avgpool = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten())\n","        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n","        self.fc = nn.Linear(Blist[2] * block.expansion*2, num_classes)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None, is_first=True,\n","                    dropblock_prob=0.0):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            down_layers = []\n","            if self.avg_down:\n","                if dilation == 1:\n","                    down_layers.append(nn.AvgPool1d(stride, stride, ceil_mode=True, count_include_pad=False))\n","                else:\n","                    down_layers.append(nn.AvgPool1d(1, 1, ceil_mode=True, count_include_pad=False))\n","                down_layers.append(nn.Conv1d(self.inplanes, planes * block.expansion, 1, 1, 0, bias=False))\n","            else:\n","                down_layers.append(nn.Conv1d(self.inplanes, planes * block.expansion, 1, stride, 0, bias=False))\n","\n","            down_layers.append(norm_layer(planes * block.expansion))\n","            downsample = nn.Sequential(*down_layers)\n","\n","        layers = []\n","        if dilation == 1 or dilation == 2:\n","            layers.append(\n","                block(\n","                    self.inplanes,\n","                    planes,\n","                    stride,\n","                    downsample=downsample,\n","                    radix=self.radix,\n","                    cardinality=self.cardinality,\n","                    bottleneck_width=self.bottleneck_width,\n","                    avd=self.avd,\n","                    avd_first=self.avd_first,\n","                    dilation=1,\n","                    is_first=is_first,\n","                    norm_layer=norm_layer,\n","                    last_gamma=self.last_gamma,\n","                    dropblock_prob=dropblock_prob\n","                )\n","            )\n","        elif dilation == 4:\n","            layers.append(\n","                block(\n","                    self.inplanes,\n","                    planes,\n","                    stride,\n","                    downsample=downsample,\n","                    radix=self.radix,\n","                    cardinality=self.cardinality,\n","                    bottleneck_width=self.bottleneck_width,\n","                    avd=self.avd,\n","                    avd_first=self.avd_first,\n","                    dilation=2,\n","                    is_first=is_first,\n","                    norm_layer=norm_layer,\n","                    last_gamma=self.last_gamma,\n","                    dropblock_prob=dropblock_prob\n","                )\n","            )\n","        else:\n","            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n","\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(\n","                block(\n","                    self.inplanes,\n","                    planes,\n","                    radix=self.radix,\n","                    cardinality=self.cardinality,\n","                    bottleneck_width=self.bottleneck_width,\n","                    avd=self.avd,\n","                    avd_first=self.avd_first,\n","                    dilation=dilation,\n","                    norm_layer=norm_layer,\n","                    last_gamma=self.last_gamma,\n","                    dropblock_prob=dropblock_prob\n","                )\n","            )\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = x.permute(0, -1, 1)\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        \n","        x = self.avgpool(x)\n","        \n","        if self.drop:\n","            x = self.drop(x)\n","        x = self.fc(x)\n","\n","        return x\n","\n","\n","def resnest_t(inchannels,block, **kwargs):\n","    model = ResNeSt1d(\n","        inchannels,\n","        block,\n","        [2, 2, 3, 3],\n","        radix=4,\n","        groups=4,\n","        bottleneck_width=8,\n","        deep_stem=True,\n","        #wisdm = 16, pamp2 = 32\n","        stem_width=32,\n","        avg_down=True,\n","        avd=True,\n","        avd_first=False,\n","        **kwargs,\n","    )\n","    return model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T06:21:04.370245Z","iopub.status.busy":"2024-02-11T06:21:04.369862Z","iopub.status.idle":"2024-02-11T06:21:04.395549Z","shell.execute_reply":"2024-02-11T06:21:04.394739Z","shell.execute_reply.started":"2024-02-11T06:21:04.370195Z"},"trusted":true},"outputs":[],"source":["import copy\n","import time\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import pandas as pd\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, x_path, y_path, transform=None):\n","        self.x_data = np.load(x_path)\n","        self.y_data = np.load(y_path)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.x_data)\n","\n","    def __getitem__(self, idx):\n","        x = self.x_data[idx]\n","        y = self.y_data[idx]\n","\n","        if self.transform:\n","            x = self.transform(x)\n","\n","        return x, y\n","#删除1\n","class LabelSmoothingCrossEntropy(nn.Module):\n","    \"\"\" NLL loss with label smoothing.\n","    \"\"\"\n","    def __init__(self, smoothing=0.1):\n","        super(LabelSmoothingCrossEntropy, self).__init__()\n","        assert smoothing < 1.0\n","        self.smoothing = smoothing\n","        self.confidence = 1. - smoothing\n","\n","    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n","        logprobs = F.log_softmax(input, dim=-1)\n","        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1).to(torch.int64))\n","        nll_loss = nll_loss.squeeze(1)\n","        smooth_loss = -logprobs.mean(dim=-1)\n","        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n","        return loss.mean()\n","class ClassBalancedLoss(nn.Module):\n","\n","    def __init__(self, num_classes, beta):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.beta = beta\n","\n","    def forward(self, inputs, targets):\n","        inputs = inputs.float()\n","        targets = targets.long()\n","        loss = F.cross_entropy(inputs, targets)\n","\n","        return self.beta * loss\n","\n","#删除2\n","class FocalLoss(nn.Module):\n","\n","    def __init__(self, alpha=0.25, gamma=2):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        inputs = inputs.float()\n","        targets = targets.long()\n","\n","        ce_loss = F.cross_entropy(inputs, targets)\n","        pt = torch.exp(-ce_loss)\n","\n","        focal_loss = ((1-pt)**self.gamma * ce_loss)\n","\n","        return focal_loss\n","#删除3\n","class MultiLoss(nn.Module):\n","    def __init__(self, num_classes, beta, weights, smoothing=0.15):\n","        super().__init__()\n","\n","        # Replace ClassBalancedLoss with LabelSmoothingLoss\n","        self.label_smoothing_loss = LabelSmoothingCrossEntropy(smoothing=0.1)\n","        self.focal_loss = FocalLoss()\n","        self.ce_loss=ClassBalancedLoss(num_classes,beta)\n","        self.weights = weights\n","\n","    def forward(self, inputs, targets):\n","        loss1 = self.weights[0] * self.label_smoothing_loss(inputs, targets)\n","        loss2 = self.weights[1] * self.focal_loss(inputs, targets)\n","        loss3 = self.weights[2] * self.ce_loss(inputs, targets)\n","        return loss1 + loss2 + loss3\n","    #删除4\n","    def update_weights(self, epoch, performance_metrics):\n","        # 根据验证准确率调整权重\n","        task1_acc = performance_metrics.get(\"task1\", 0.0)\n","        task2_acc = performance_metrics.get(\"task2\", 0.0)\n","        task3_acc = performance_metrics.get(\"task3\", 0.0)\n","\n","        # 根据验证准确率的倒数来调整权重\n","        # 准确率越高的任务将获得较小的权重，反之亦然\n","        self.weights[0] = 0.5*(1-self.weights[1])\n","        self.weights[1] = 1-(1.0 / (task1_acc + 1e-8)-1)-a\n","        self.weights[2] = 0.5*(1-self.weights[1])\n","\n","        print(f\"Epoch {epoch}: 更新权重 - 任务1: {self.weights[0]}, 任务2: {self.weights[1]},任务3: {self.weights[2]}\")\n","\n","class MultiLoss_Withoutweight(nn.Module):\n","    def __init__(self, num_classes, beta, weights, smoothing=0.15):\n","        super().__init__()\n","\n","        # Replace ClassBalancedLoss with LabelSmoothingLoss\n","        self.label_smoothing_loss = LabelSmoothingCrossEntropy(smoothing=0.1)\n","        self.focal_loss = FocalLoss()\n","        self.ce_loss=ClassBalancedLoss(num_classes,beta)\n","        self.weights = weights\n","\n","    def forward(self, inputs, targets):\n","        loss1 = self.label_smoothing_loss(inputs, targets)\n","        loss2 = self.focal_loss(inputs, targets)\n","        loss3 = self.ce_loss(inputs, targets)\n","        return loss1 + loss2 + loss3\n","\n","        \n","        \n"," #删除5       \n","def compute_validation_accuracy(model, val_dataloader, device):\n","    model.eval()\n","    corrects = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in val_dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            corrects += (predicted == labels).sum().item()\n","\n","    accuracy = corrects / total\n","    return accuracy"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T06:21:04.397104Z","iopub.status.busy":"2024-02-11T06:21:04.396766Z","iopub.status.idle":"2024-02-11T06:21:04.423874Z","shell.execute_reply":"2024-02-11T06:21:04.422906Z","shell.execute_reply.started":"2024-02-11T06:21:04.397074Z"},"trusted":true},"outputs":[],"source":["pamp2_b =128\n","wisdm_b = 512\n","\n","def train_val_data_process():\n","    # 训练数据集的路径\n","\n","\n","    train_dataset = CustomDataset(x_train_path, y_train_path)\n","\n","    # 将数据集拆分为训练集和验证集\n","    train_size = int(0.8 * len(train_dataset))\n","    val_size = len(train_dataset) - train_size\n","    train_data, val_data = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n","\n","    # 定义训练集和验证集的数据加载器\n","\n","    train_dataloader = DataLoader(train_data, batch_size=pamp2_b, shuffle=True, num_workers=2)#wisdm  batch_size=256  pamp2  batch_size=128\n","    val_dataloader = DataLoader(val_data, batch_size=pamp2_b, shuffle=True, num_workers=2)#wisdm  batch_size=256  pamp2  batch_size=128\n","\n","    return train_dataloader, val_dataloader\n","\n","\n","def train_model_process(model, train_dataloader, val_dataloader, num_epochs,criterion):\n","    # 设定训练所用到的设备，有GPU用GPU没有GPU用CPU\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # 使用Adam优化器，学习率为0.001\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)#wisdm0.005  pamp2  0.01\n","\n","    # 将模型放入到训练设备中\n","    model = model.to(device)\n","    # 复制当前模型的参数\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    # 初始化参数\n","    # 最高准确度\n","    best_acc = 0.0\n","    # 训练集损失列表\n","    train_loss_all = []\n","    # 验证集损失列表\n","    val_loss_all = []\n","    # 训练集准确度列表\n","    train_acc_all = []\n","    # 验证集准确度列表\n","    val_acc_all = []\n","    # 当前时间\n","    since = time.time()\n","    #删除6\n","    multi_loss = MultiLoss(num_classes, beta, weights)\n","\n","\n","    for epoch in range(num_epochs):\n","        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n","        print(\"-\" * 10)\n","\n","        # 初始化参数\n","        # 训练集损失函数\n","        train_loss = 0.0\n","        # 训练集准确度\n","        train_corrects = 0\n","        # 验证集损失函数\n","        val_loss = 0.0\n","        # 验证集准确度\n","        val_corrects = 0\n","        # 训练集样本数量\n","        train_num = 0\n","        # 验证集样本数量\n","        val_num = 0\n","\n","        # 对每一个mini-batch训练和计算\n","        for step, (b_x, b_y) in enumerate(train_dataloader):\n","            # 将特征放入到训练设备中\n","            b_x = b_x.to(device)\n","            # 将标签放入到训练设备中\n","            b_y = b_y.to(device)\n","            # 设置模型为训练模式\n","            model.train()\n","\n","            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n","            output = model(b_x)\n","            # 查找每一行中最大值对应的行标\n","            pre_lab = torch.argmax(output, dim=1)\n","            # 计算每一个batch的损失函数\n","            loss = criterion(output, b_y)\n","\n","            # 将梯度初始化为0\n","            optimizer.zero_grad()\n","            # 反向传播计算\n","            loss.backward()\n","            # 根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值的作用\n","            optimizer.step()\n","            # 对损失函数进行累加\n","            train_loss += loss.item() * b_x.size(0)\n","            # 如果预测正确，则准确度train_corrects加1\n","            train_corrects += torch.sum(pre_lab == b_y.data)\n","            # 当前用于训练的样本数量\n","            train_num += b_x.size(0)\n","        for step, (b_x, b_y) in enumerate(val_dataloader):\n","            # 将特征放入到验证设备中\n","            b_x = b_x.to(device)\n","            # 将标签放入到验证设备中\n","            b_y = b_y.to(device)\n","            # 设置模型为评估模式\n","            model.eval()\n","            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n","            output = model(b_x)\n","            # 查找每一行中最大值对应的行标\n","            pre_lab = torch.argmax(output, dim=1)\n","            # 计算每一个batch的损失函数\n","            loss = criterion(output, b_y)\n","\n","            # 对损失函数进行累加\n","            val_loss += loss.item() * b_x.size(0)\n","            # 如果预测正确，则准确度train_corrects加1\n","            val_corrects += torch.sum(pre_lab == b_y.data)\n","            # 当前用于验证的样本数量\n","            val_num += b_x.size(0)\n","\n","        # 计算并保存每一次迭代的loss值和准确率\n","        # 计算并保存训练集的loss值\n","        train_loss_all.append(train_loss / train_num)\n","        # 计算并保存训练集的准确率\n","        train_acc_all.append(train_corrects.double().item() / train_num)\n","\n","        # 计算并保存验证集的loss值\n","        val_loss_all.append(val_loss / val_num)\n","        # 计算并保存验证集的准确率\n","        val_acc_all.append(val_corrects.double().item() / val_num)\n","\n","        print(\"{} train loss:{:.4f} train acc: {:.4f}\".format(epoch, train_loss_all[-1], train_acc_all[-1]))\n","        print(\"{} val loss:{:.4f} val acc: {:.4f}\".format(epoch, val_loss_all[-1], val_acc_all[-1]))\n","\n","        if val_acc_all[-1] > best_acc:\n","            # 保存当前最高准确度\n","            best_acc = val_acc_all[-1]\n","            # 保存当前最高准确度的模型参数\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        # 计算训练和验证的耗时\n","        time_use = time.time() - since\n","        print(\"训练和验证耗费的时间{:.0f}m{:.0f}s\".format(time_use // 60, time_use % 60))\n","        #删除7\n","        val_acc_task1 = compute_validation_accuracy(model, val_dataloader, device)\n","        val_acc_task2 = compute_validation_accuracy(model, val_dataloader, device)\n","        val_acc_task3 = compute_validation_accuracy(model, val_dataloader, device)\n","        multi_loss.update_weights(epoch, {\"task1\": val_acc_task1, \"task2\": val_acc_task2, \"task3\": val_acc_task3})\n","        multi_loss = MultiLoss(num_classes, beta, multi_loss.weights)\n","\n"," \n","\n","    # 选择最优参数，保存最优参数的模型\n","    print(best_acc)\n","    train_process = pd.DataFrame(data={\"epoch\": range(num_epochs),\n","                                       \"train_loss_all\": train_loss_all,\n","                                       \"val_loss_all\": val_loss_all,\n","                                       \"train_acc_all\": train_acc_all,\n","                                       \"val_acc_all\": val_acc_all, })\n","\n","    return train_process\n","\n","\n","def matplot_acc_loss(train_process):\n","    # 显示每一次迭代后的训练集和验证集的损失函数和准确率\n","    plt.figure(figsize=(12, 4))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_process['epoch'], train_process.train_loss_all, \"ro-\", label=\"Train loss\")\n","    plt.plot(train_process['epoch'], train_process.val_loss_all, \"bs-\", label=\"Val loss\")\n","    plt.legend()\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_process['epoch'], train_process.train_acc_all, \"ro-\", label=\"Train acc\")\n","    plt.plot(train_process['epoch'], train_process.val_acc_all, \"bs-\", label=\"Val acc\")\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"acc\")\n","    plt.legend()\n","    plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T06:21:04.425257Z","iopub.status.busy":"2024-02-11T06:21:04.424958Z","iopub.status.idle":"2024-02-11T06:23:31.743996Z","shell.execute_reply":"2024-02-11T06:23:31.742485Z","shell.execute_reply.started":"2024-02-11T06:21:04.425206Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0/49\n","----------\n","0 train loss:1.6892 train acc: 0.5277\n","0 val loss:2.7913 val acc: 0.5027\n","训练和验证耗费的时间0m2s\n","Epoch 0: 更新权重 - 任务1: 0.25, 任务2: -0.4391807950529259,任务3: 0.7195903975264629\n","Epoch 1/49\n","----------\n","1 train loss:0.7801 train acc: 0.7630\n","1 val loss:1.2449 val acc: 0.6791\n","训练和验证耗费的时间0m5s\n","Epoch 1: 更新权重 - 任务1: 0.7195903975264629, 任务2: 0.0774599759171512,任务3: 0.46127001204142437\n","Epoch 2/49\n","----------\n","2 train loss:1.5085 train acc: 0.7113\n","2 val loss:1.9189 val acc: 0.7824\n","训练和验证耗费的时间0m8s\n","Epoch 2: 更新权重 - 任务1: 0.46127001204142437, 任务2: 0.2719463917066038,任务3: 0.3640268041466981\n","Epoch 3/49\n","----------\n","3 train loss:0.7942 train acc: 0.8080\n","3 val loss:0.8747 val acc: 0.7855\n","训练和验证耗费的时间0m11s\n","Epoch 3: 更新权重 - 任务1: 0.3640268041466981, 任务2: 0.27700298356426484,任务3: 0.36149850821786755\n","Epoch 4/49\n","----------\n","4 train loss:0.5862 train acc: 0.8368\n","4 val loss:0.4506 val acc: 0.9106\n","训练和验证耗费的时间0m14s\n","Epoch 4: 更新权重 - 任务1: 0.36149850821786755, 任务2: 0.4518771451645402,任务3: 0.2740614274177299\n","Epoch 5/49\n","----------\n","5 train loss:0.4975 train acc: 0.8611\n","5 val loss:0.4112 val acc: 0.9145\n","训练和验证耗费的时间0m17s\n","Epoch 5: 更新权重 - 任务1: 0.2740614274177299, 任务2: 0.456542068031269,任务3: 0.27172896598436547\n","Epoch 6/49\n","----------\n","6 train loss:0.3446 train acc: 0.8982\n","6 val loss:0.3255 val acc: 0.9215\n","训练和验证耗费的时间0m20s\n","Epoch 6: 更新权重 - 任务1: 0.27172896598436547, 任务2: 0.46483980941484954,任务3: 0.2675800952925752\n","Epoch 7/49\n","----------\n","7 train loss:0.3253 train acc: 0.9132\n","7 val loss:0.2855 val acc: 0.9378\n","训练和验证耗费的时间0m23s\n","Epoch 7: 更新权重 - 任务1: 0.2675800952925752, 任务2: 0.48371997822951446,任务3: 0.25814001088524274\n","Epoch 8/49\n","----------\n","8 train loss:0.2882 train acc: 0.9269\n","8 val loss:0.2828 val acc: 0.9347\n","训练和验证耗费的时间0m26s\n","Epoch 8: 更新权重 - 任务1: 0.25814001088524274, 任务2: 0.4801745750362871,任务3: 0.25991271248185643\n","Epoch 9/49\n","----------\n","9 train loss:0.2902 train acc: 0.9269\n","9 val loss:0.2725 val acc: 0.9270\n","训练和验证耗费的时间0m28s\n","Epoch 9: 更新权重 - 任务1: 0.25991271248185643, 任务2: 0.47120705271086777,任务3: 0.2643964736445661\n","Epoch 10/49\n","----------\n","10 train loss:0.3028 train acc: 0.9219\n","10 val loss:0.4232 val acc: 0.8353\n","训练和验证耗费的时间0m31s\n","Epoch 10: 更新权重 - 任务1: 0.2643964736445661, 任务2: 0.3527907120075197,任务3: 0.3236046439962401\n","Epoch 11/49\n","----------\n","11 train loss:0.3561 train acc: 0.9009\n","11 val loss:0.3038 val acc: 0.9239\n","训练和验证耗费的时间0m34s\n","Epoch 11: 更新权重 - 任务1: 0.3236046439962401, 任务2: 0.467577808183998,任务3: 0.26621109590800096\n","Epoch 12/49\n","----------\n","12 train loss:0.3674 train acc: 0.9110\n","12 val loss:0.3380 val acc: 0.9270\n","训练和验证耗费的时间0m37s\n","Epoch 12: 更新权重 - 任务1: 0.26621109590800096, 任务2: 0.47120705271086777,任务3: 0.2643964736445661\n","Epoch 13/49\n","----------\n","13 train loss:0.2928 train acc: 0.9324\n","13 val loss:0.2556 val acc: 0.9456\n","训练和验证耗费的时间0m39s\n","Epoch 13: 更新权重 - 任务1: 0.2643964736445661, 任务2: 0.4924815230979975,任务3: 0.25375923845100123\n","Epoch 14/49\n","----------\n","14 train loss:0.2576 train acc: 0.9450\n","14 val loss:0.4186 val acc: 0.9044\n","训练和验证耗费的时间0m42s\n","Epoch 14: 更新权重 - 任务1: 0.25375923845100123, 任务2: 0.44432990913228027,任务3: 0.27783504543385984\n","Epoch 15/49\n","----------\n","15 train loss:0.2976 train acc: 0.9353\n","15 val loss:0.2401 val acc: 0.9588\n","训练和验证耗费的时间0m45s\n","Epoch 15: 更新权重 - 任务1: 0.27783504543385984, 任务2: 0.5070502539892734,任务3: 0.2464748730053633\n","Epoch 16/49\n","----------\n","16 train loss:0.2563 train acc: 0.9501\n","16 val loss:0.2467 val acc: 0.9518\n","训练和验证耗费的时间0m47s\n","Epoch 16: 更新权重 - 任务1: 0.2464748730053633, 任务2: 0.4993877661399017,任务3: 0.2503061169300491\n","Epoch 17/49\n","----------\n","17 train loss:0.2220 train acc: 0.9578\n","17 val loss:0.2148 val acc: 0.9596\n","训练和验证耗费的时间0m50s\n","Epoch 17: 更新权重 - 任务1: 0.2503061169300491, 任务2: 0.5078947477019391,任务3: 0.24605262614903045\n","Epoch 18/49\n","----------\n","18 train loss:0.2338 train acc: 0.9538\n","18 val loss:0.2194 val acc: 0.9604\n","训练和验证耗费的时间0m53s\n","Epoch 18: 更新权重 - 任务1: 0.24605262614903045, 任务2: 0.5087378749199383,任务3: 0.24563106254003086\n","Epoch 19/49\n","----------\n","19 train loss:0.2090 train acc: 0.9639\n","19 val loss:0.2081 val acc: 0.9604\n","训练和验证耗费的时间0m56s\n","Epoch 19: 更新权重 - 任务1: 0.24563106254003086, 任务2: 0.5087378749199383,任务3: 0.24563106254003086\n","Epoch 20/49\n","----------\n","20 train loss:0.2013 train acc: 0.9687\n","20 val loss:0.2046 val acc: 0.9650\n","训练和验证耗费的时间0m59s\n","Epoch 20: 更新权重 - 任务1: 0.24563106254003086, 任务2: 0.5137681266797942,任务3: 0.2431159366601029\n","Epoch 21/49\n","----------\n","21 train loss:0.1934 train acc: 0.9726\n","21 val loss:0.2026 val acc: 0.9650\n","训练和验证耗费的时间1m2s\n","Epoch 21: 更新权重 - 任务1: 0.2431159366601029, 任务2: 0.5137681266797942,任务3: 0.2431159366601029\n","Epoch 22/49\n","----------\n","22 train loss:0.1906 train acc: 0.9734\n","22 val loss:0.1921 val acc: 0.9681\n","训练和验证耗费的时间1m5s\n","Epoch 22: 更新权重 - 任务1: 0.2431159366601029, 任务2: 0.5170947137186928,任务3: 0.24145264314065362\n","Epoch 23/49\n","----------\n","23 train loss:0.1915 train acc: 0.9714\n","23 val loss:0.1971 val acc: 0.9666\n","训练和验证耗费的时间1m8s\n","Epoch 23: 更新权重 - 任务1: 0.24145264314065362, 任务2: 0.5154340943045526,任务3: 0.24228295284772372\n","Epoch 24/49\n","----------\n","24 train loss:0.1841 train acc: 0.9744\n","24 val loss:0.2086 val acc: 0.9635\n","训练和验证耗费的时间1m11s\n","Epoch 24: 更新权重 - 任务1: 0.24228295284772372, 任务2: 0.5120967849659794,任务3: 0.24395160751701028\n","Epoch 25/49\n","----------\n","25 train loss:0.2910 train acc: 0.9100\n","25 val loss:0.3084 val acc: 0.9083\n","训练和验证耗费的时间1m13s\n","Epoch 25: 更新权重 - 任务1: 0.24395160751701028, 任务2: 0.44905903692823806,任务3: 0.27547048153588094\n","Epoch 26/49\n","----------\n","26 train loss:0.2758 train acc: 0.9316\n","26 val loss:0.2460 val acc: 0.9371\n","训练和验证耗费的时间1m16s\n","Epoch 26: 更新权重 - 任务1: 0.27547048153588094, 任务2: 0.4828358322839163,任务3: 0.2585820838580418\n","Epoch 27/49\n","----------\n","27 train loss:0.2500 train acc: 0.9540\n","27 val loss:0.2363 val acc: 0.9549\n","训练和验证耗费的时间1m19s\n","Epoch 27: 更新权重 - 任务1: 0.2585820838580418, 任务2: 0.5028071712590496,任务3: 0.2485964143704752\n","Epoch 28/49\n","----------\n","28 train loss:0.2109 train acc: 0.9668\n","28 val loss:0.2119 val acc: 0.9611\n","训练和验证耗费的时间1m22s\n","Epoch 28: 更新权重 - 任务1: 0.2485964143704752, 任务2: 0.5095796389573242,任务3: 0.24521018052133792\n","Epoch 29/49\n","----------\n","29 train loss:0.2029 train acc: 0.9666\n","29 val loss:0.2098 val acc: 0.9573\n","训练和验证耗费的时间1m25s\n","Epoch 29: 更新权重 - 任务1: 0.24521018052133792, 任务2: 0.50535715376993,任务3: 0.24732142311503502\n","Epoch 30/49\n","----------\n","30 train loss:0.1949 train acc: 0.9679\n","30 val loss:0.1987 val acc: 0.9650\n","训练和验证耗费的时间1m28s\n","Epoch 30: 更新权重 - 任务1: 0.24732142311503502, 任务2: 0.5137681266797942,任务3: 0.2431159366601029\n","Epoch 31/49\n","----------\n","31 train loss:0.1932 train acc: 0.9712\n","31 val loss:0.2028 val acc: 0.9643\n","训练和验证耗费的时间1m30s\n","Epoch 31: 更新权重 - 任务1: 0.2431159366601029, 任务2: 0.5129331292079378,任务3: 0.24353343539603112\n","Epoch 32/49\n","----------\n","32 train loss:0.2239 train acc: 0.9540\n","32 val loss:0.2094 val acc: 0.9611\n","训练和验证耗费的时间1m33s\n","Epoch 32: 更新权重 - 任务1: 0.24353343539603112, 任务2: 0.5095796389573242,任务3: 0.24521018052133792\n","Epoch 33/49\n","----------\n","33 train loss:0.1879 train acc: 0.9728\n","33 val loss:0.1940 val acc: 0.9681\n","训练和验证耗费的时间1m36s\n","Epoch 33: 更新权重 - 任务1: 0.24521018052133792, 任务2: 0.5170947137186928,任务3: 0.24145264314065362\n","Epoch 34/49\n","----------\n","34 train loss:0.1842 train acc: 0.9763\n","34 val loss:0.1875 val acc: 0.9697\n","训练和验证耗费的时间1m39s\n","Epoch 34: 更新权重 - 任务1: 0.24145264314065362, 任务2: 0.5187500106347656,任务3: 0.24062499468261722\n","Epoch 35/49\n","----------\n","35 train loss:0.1757 train acc: 0.9792\n","35 val loss:0.1984 val acc: 0.9643\n","训练和验证耗费的时间1m42s\n","Epoch 35: 更新权重 - 任务1: 0.24062499468261722, 任务2: 0.5129331292079378,任务3: 0.24353343539603112\n","Epoch 36/49\n","----------\n","36 train loss:0.1905 train acc: 0.9709\n","36 val loss:0.1966 val acc: 0.9674\n","训练和验证耗费的时间1m45s\n","Epoch 36: 更新权重 - 任务1: 0.24353343539603112, 任务2: 0.5162650709270431,任务3: 0.24186746453647845\n","Epoch 37/49\n","----------\n","37 train loss:0.1899 train acc: 0.9744\n","37 val loss:0.1925 val acc: 0.9674\n","训练和验证耗费的时间1m48s\n","Epoch 37: 更新权重 - 任务1: 0.24186746453647845, 任务2: 0.5162650709270431,任务3: 0.24186746453647845\n","Epoch 38/49\n","----------\n","38 train loss:0.1790 train acc: 0.9777\n","38 val loss:0.1853 val acc: 0.9697\n","训练和验证耗费的时间1m51s\n","Epoch 38: 更新权重 - 任务1: 0.24186746453647845, 任务2: 0.5187500106347656,任务3: 0.24062499468261722\n","Epoch 39/49\n","----------\n","39 train loss:0.1951 train acc: 0.9693\n","39 val loss:0.2022 val acc: 0.9643\n","训练和验证耗费的时间1m54s\n","Epoch 39: 更新权重 - 任务1: 0.24062499468261722, 任务2: 0.5129331292079378,任务3: 0.24353343539603112\n","Epoch 40/49\n","----------\n","40 train loss:0.2310 train acc: 0.9503\n","40 val loss:0.2071 val acc: 0.9627\n","训练和验证耗费的时间1m57s\n","Epoch 40: 更新权重 - 任务1: 0.24353343539603112, 任务2: 0.5112590906929748,任务3: 0.2443704546535126\n","Epoch 41/49\n","----------\n","41 train loss:0.2025 train acc: 0.9664\n","41 val loss:0.2031 val acc: 0.9588\n","训练和验证耗费的时间2m0s\n","Epoch 41: 更新权重 - 任务1: 0.2443704546535126, 任务2: 0.5070502539892734,任务3: 0.2464748730053633\n","Epoch 42/49\n","----------\n","42 train loss:0.1874 train acc: 0.9751\n","42 val loss:0.1929 val acc: 0.9713\n","训练和验证耗费的时间2m3s\n","Epoch 42: 更新权重 - 任务1: 0.2464748730053633, 任务2: 0.5204000106007616,任务3: 0.2397999946996192\n","Epoch 43/49\n","----------\n","43 train loss:0.1977 train acc: 0.9695\n","43 val loss:0.1908 val acc: 0.9736\n","训练和验证耗费的时间2m6s\n","Epoch 43: 更新权重 - 任务1: 0.2397999946996192, 任务2: 0.5228651342531732,任务3: 0.23856743287341342\n","Epoch 44/49\n","----------\n","44 train loss:0.1827 train acc: 0.9744\n","44 val loss:0.1844 val acc: 0.9736\n","训练和验证耗费的时间2m9s\n","Epoch 44: 更新权重 - 任务1: 0.23856743287341342, 任务2: 0.5228651342531732,任务3: 0.23856743287341342\n","Epoch 45/49\n","----------\n","45 train loss:0.1722 train acc: 0.9798\n","45 val loss:0.1838 val acc: 0.9705\n","训练和验证耗费的时间2m12s\n","Epoch 45: 更新权重 - 任务1: 0.23856743287341342, 任务2: 0.5195756711461659,任务3: 0.24021216442691706\n","Epoch 46/49\n","----------\n","46 train loss:0.1740 train acc: 0.9786\n","46 val loss:0.1802 val acc: 0.9744\n","训练和验证耗费的时间2m15s\n","Epoch 46: 更新权重 - 任务1: 0.24021216442691706, 任务2: 0.5236842210595567,任务3: 0.23815788947022165\n","Epoch 47/49\n","----------\n","47 train loss:0.1816 train acc: 0.9757\n","47 val loss:0.1842 val acc: 0.9705\n","训练和验证耗费的时间2m18s\n","Epoch 47: 更新权重 - 任务1: 0.23815788947022165, 任务2: 0.5195756711461659,任务3: 0.24021216442691706\n","Epoch 48/49\n","----------\n","48 train loss:0.1701 train acc: 0.9829\n","48 val loss:0.1802 val acc: 0.9728\n","训练和验证耗费的时间2m21s\n","Epoch 48: 更新权重 - 任务1: 0.24021216442691706, 任务2: 0.5220447390014251,任务3: 0.23897763049928744\n","Epoch 49/49\n","----------\n","49 train loss:0.1725 train acc: 0.9827\n","49 val loss:0.1821 val acc: 0.9736\n","训练和验证耗费的时间2m24s\n","Epoch 49: 更新权重 - 任务1: 0.23897763049928744, 任务2: 0.5228651342531732,任务3: 0.23856743287341342\n","0.9743589743589743\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 1200x400 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# search pamp to find the differences and change\n","num_classes = 12 #pamp2 =12 wisdm =6\n","inchannels = 36 #pamp2 =36 wisdm =3\n","\n","#1\n","#model_n_casa = resnest_t(inchannels=inchannels,num_classes=num_classes,block = ResNeStBottleneck_WithoutCASA)\n","#2\n","#model_n_ca = resnest_t(inchannels=inchannels,num_classes=num_classes,block = ResNeStBottleneck_WithoutCA)\n","#3\n","#model_n_sa = resnest_t(inchannels=inchannels,num_classes=num_classes,block = ResNeStBottleneck_WithoutSA)\n","#4\n","model = resnest_t(inchannels=inchannels,num_classes=num_classes,block = ResNeStBottleneck)\n","\n","model = model\n","if __name__ == '__main__':\n","    # 加载需要的模型\n","    \n","    # 加载数据集\n","    train_data, val_data = train_val_data_process()\n","    # 利用现有的模型进行模型的训练\n","      # 替换为你的类别总数\n","    beta = 1  # 平衡系数  #wisdm\n","    #beta=0.8  #pamp2\n","    #weights = [0.1, 0.8,0.1] #wisdm\n","    a=0.45\n","    weights = [0.25, 0.5,0.25] #pamp2\n","    #4\n","    criterion1 = nn.CrossEntropyLoss()\n","    #5\n","    criterion2 = MultiLoss_Withoutweight(num_classes, beta, weights)\n","    #不测\n","    criterion3 = MultiLoss(num_classes, beta, weights,smoothing=0.1)#smoothing=0.15  #pamp2\n","\n","    train_process = train_model_process(model, train_data,val_data, num_epochs=50,criterion = criterion3)\n","    matplot_acc_loss(train_process)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model, 'res_pamap2100_model.pt')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.status.busy":"2024-02-11T06:23:31.745275Z","iopub.status.idle":"2024-02-11T06:23:31.745648Z","shell.execute_reply":"2024-02-11T06:23:31.745490Z","shell.execute_reply.started":"2024-02-11T06:23:31.745473Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 0.4076, Test Acc: 89.90%, G-mean: 0.9835\n","              precision    recall  f1-score   support\n","\n","           0     0.8750    1.0000    0.9333        91\n","           1     0.9250    0.7115    0.8043       104\n","           2     0.9189    0.8000    0.8553        85\n","           3     1.0000    0.9677    0.9836       124\n","           4     1.0000    0.9158    0.9560        95\n","           5     0.8614    0.9158    0.8878        95\n","           6     0.9528    0.9902    0.9712       102\n","           7     0.8500    0.9273    0.8870        55\n","           8     0.9744    0.7917    0.8736        48\n","           9     0.8172    0.8085    0.8128        94\n","          10     0.8092    0.9609    0.8786       128\n","          11     0.8235    0.9655    0.8889        29\n","\n","    accuracy                         0.8990      1050\n","   macro avg     0.9006    0.8962    0.8944      1050\n","weighted avg     0.9045    0.8990    0.8979      1050\n","\n","G-mean: 0.9835\n"]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","import torch.nn as nn\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, x_path, y_path, transform=None):\n","        self.x_data = np.load(x_path)\n","        self.y_data = np.load(y_path)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.x_data)\n","\n","    def __getitem__(self, idx):\n","        x = self.x_data[idx]\n","        y = self.y_data[idx]\n","\n","        if self.transform:\n","            x = self.transform(x)\n","\n","        return x, y\n","\n","def test_final(model, test_dataloader):\n","    device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n","    model = model.to(device)\n","    criterion1 = nn.CrossEntropyLoss()\n","    num_classes = 6\n","    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)  \n","\n","    model.eval()\n","    test_loss = 0.0\n","    test_correct = 0\n","    test_total = 0\n","    y_true = []\n","    y_pred = []\n","\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(test_dataloader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion1(outputs, labels)\n","            _, predicted = torch.max(outputs.data, 1)\n","            test_correct += (predicted == labels).sum().item()\n","            test_total += labels.size(0)\n","            test_loss += loss.item()\n","            y_true.extend(labels.tolist())\n","            y_pred.extend(predicted.tolist())\n","            conf_matrix += confusion_matrix(labels.cpu(), predicted.cpu(), labels=range(num_classes))\n","\n","    report = classification_report(y_true, y_pred,digits=4)\n","    test_acc = 100.0 * test_correct / test_total\n","    test_loss = test_loss / len(test_dataloader)\n","\n","    g_mean = np.sqrt(np.diag(conf_matrix) / np.sum(conf_matrix, axis=1))\n","    g_mean = np.mean(g_mean)\n","    report += '\\nG-mean: {:.4f}'.format(g_mean)\n","    print('Test Loss: {:.4f}, Test Acc: {:.2f}%, G-mean: {:.4f}'.format(test_loss, test_acc, g_mean))\n","    print(report)\n","\n","\n","def test_data_process():\n","    test_data = CustomDataset(x_test_path, y_test_path)\n","    test_dataloader = DataLoader(test_data, batch_size=pamp2_b, shuffle=True, num_workers=2)\n","    return test_dataloader\n","\n","\n","def test_model_process(model, test_dataloader):\n","    device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n","    model = model.to(device)\n","    classes = ['Downstairs', 'Jogging', 'Sitting', 'Standing', 'Upstairs', 'Walking']\n","\n","    test_corrects = 0.0\n","    test_num = 0\n","\n","    with torch.no_grad():\n","        for test_data_x, test_data_y in test_dataloader:\n","            test_data_x = test_data_x.to(device)\n","            test_data_y = test_data_y.to(device)\n","            model.eval()\n","            output = model(test_data_x)\n","            pre_lab = torch.argmax(output, dim=1)\n","            test_corrects += torch.sum(pre_lab == test_data_y.data)\n","            test_num += test_data_x.size(0)\n","\n","    test_acc = test_corrects.double().item() / test_num\n","    print(\"测试的准确率为：\", test_acc)\n","\n","\n","\n","if __name__ == \"__main__\":\n","    model = torch.load('res_pamap2100_model.pt')\n","    test_dataloader = test_data_process()\n","    test_final(model, test_dataloader)\n","  \n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4183646,"sourceId":7226790,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
