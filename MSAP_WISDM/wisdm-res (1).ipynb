{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4950975,"sourceType":"datasetVersion","datasetId":2871088},{"sourceId":7226790,"sourceType":"datasetVersion","datasetId":4183646},{"sourceId":7491965,"sourceType":"datasetVersion","datasetId":4362083}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-11T06:24:35.017278Z","iopub.execute_input":"2024-02-11T06:24:35.017615Z","iopub.status.idle":"2024-02-11T06:24:35.395984Z","shell.execute_reply.started":"2024-02-11T06:24:35.017590Z","shell.execute_reply":"2024-02-11T06:24:35.395044Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/wisdm-data/wisdm/y_train.npy\n/kaggle/input/wisdm-data/wisdm/y_test.npy\n/kaggle/input/wisdm-data/wisdm/x_test.npy\n/kaggle/input/wisdm-data/wisdm/x_train.npy\n/kaggle/input/933333/93.33.pth\n/kaggle/input/pamap22/PAMAP2/y_train.npy\n/kaggle/input/pamap22/PAMAP2/y_test.npy\n/kaggle/input/pamap22/PAMAP2/x_test.npy\n/kaggle/input/pamap22/PAMAP2/x_train.npy\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SEModule(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Conv1d(channels, channels // reduction, kernel_size=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv1d(channels // reduction, channels, kernel_size=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.fc1(y)\n        y = self.relu(y)\n        y = self.fc2(y)\n        y = self.sigmoid(y)\n        return x * y\n\n\nclass ChannelAttention1d(nn.Module):\n\n    def __init__(self, in_channels, ratio=16):\n        super(ChannelAttention1d, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.max_pool = nn.AdaptiveMaxPool1d(1)\n\n        self.fc1 = nn.Conv1d(in_channels, in_channels//16, 1, bias=False)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Conv1d(in_channels//16, in_channels, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention1d(nn.Module):\n\n    def __init__(self, kernel_size=3):\n        super(SpatialAttention1d, self).__init__()\n        self.conv = nn.Conv1d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv(x)\n        return self.sigmoid(x)\n\nclass rSoftMax(nn.Module):\n    def __init__(self, radix, cardinality):\n        super().__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n\n    def forward(self, x):\n        if self.radix > 1:\n            batch = x.size(0)\n            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n            x = F.softmax(x, dim=1)\n            x = x.reshape(batch, -1)\n        else:\n            x = torch.sigmoid(x)\n        return x\n\n\nclass DropBlock1D(object):\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass SplAtConv1d(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1,\n            groups=1,\n            bias=True,\n            radix=2,\n            reduction_factor=4,\n            norm_layer=None,\n            dropblock_prob=0.0,\n            **kwargs,\n\n    ):\n        super().__init__()\n\n        self.dropblock_prob = dropblock_prob\n        inter_channels = max(in_channels * radix // reduction_factor, 32)\n        self.radix = radix\n        self.cardinality = groups\n        self.channels = channels\n\n        self.conv = nn.Conv1d(\n            in_channels,\n            channels * radix,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups=groups * radix,\n            bias=bias,\n            **kwargs,\n        )\n        self.bn0 = norm_layer(self.channels * radix)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc1 = nn.Conv1d(self.channels, inter_channels, 1, groups=self.cardinality)\n        self.bn1 = norm_layer(inter_channels)\n        self.fc2 = nn.Conv1d(inter_channels, self.channels * radix, 1, groups=self.cardinality)\n        if dropblock_prob > 0.0:\n            self.dropblock = DropBlock1D(dropblock_prob, 3)\n        self.rsoftmax = rSoftMax(radix, self.cardinality)\n        self.se = SEModule(in_channels)\n        self.ca1 = ChannelAttention1d(in_channels)\n        self.sa1 = SpatialAttention1d()\n\n    def forward(self, x):\n        #x = self.ca1(x) * x\n        #x=  self.sa1(x) * x\n        #x = self.se(x) * x\n        x = self.conv(x)\n        x = self.bn0(x)\n        if self.dropblock_prob > 0.0:\n            x = self.dropblock(x)\n        x = self.relu(x)\n\n        batch, rchannel = x.shape[:2]\n        if self.radix > 1:\n            splited = torch.split(x, int(rchannel // self.radix), dim=1)\n            gap = sum(splited)\n        else:\n            gap = x\n        #se放在这个位置结果非常好\n        #位置1，通道注意力\n        gap = self.ca1(gap) * gap\n        #gap = self.sa1(gap) * gap\n        #gap = self.se(gap) * gap\n        gap = F.adaptive_avg_pool1d(gap, 1)\n        gap = self.fc1(gap)\n        gap = self.bn1(gap)\n        gap = self.relu(gap)\n\n        atten = self.fc2(gap)\n        atten = self.rsoftmax(atten).view(batch, -1, 1)\n\n        if self.radix > 1:\n            attens = torch.split(atten, int(rchannel // self.radix), dim=1)\n            outs = []\n            for att, split in zip(attens, splited):\n                outs.append(att * split)\n            out = sum(outs)\n        else:\n            out = atten * x\n       # out = self.se(out) * out\n        #out = self.ca1(out) * out\n        #out = self.sa1(out) * out\n        return out.contiguous()\n\n\n\n\n\nclass ResNeStBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            radix=1,\n            cardinality=1,\n            bottleneck_width=64,\n            avd=False,\n            avd_first=False,\n            dilation=1,\n            is_first=False,\n            norm_layer=None,\n            last_gamma=False,\n            dropblock_prob=0.0\n    ):\n        super().__init__()\n        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n\n        self.conv1 = nn.Conv1d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.radix = radix\n        self.avd = avd and (stride > 1 or is_first)\n        self.avd_first = avd_first\n        self.dropblock_prob = dropblock_prob\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool1d(3, stride, padding=1)\n            stride = 1\n        if dropblock_prob > 0.0:\n            self.dropblock1 = DropBlock1D(dropblock_prob, 3)\n            if radix == 1:\n                self.dropblock2 = DropBlock1D(dropblock_prob, 3)\n            self.dropblock3 = DropBlock1D(dropblock_prob, 3)\n\n        if radix >= 1:\n            self.conv2 = SplAtConv1d(\n                group_width,\n                group_width,\n                kernel_size=3,\n                stride=stride,\n                padding=dilation,\n                dilation=dilation,\n                groups=cardinality,\n                bias=False,\n                radix=radix,\n                norm_layer=norm_layer,\n                dropblock_prob=dropblock_prob\n            )\n        else:\n            self.conv2 = nn.Conv1d(\n                group_width,\n                group_width,\n                kernel_size=3,\n                stride=stride,\n                padding=dilation,\n                dilation=dilation,\n                groups=cardinality,\n                bias=False,\n            )\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv1d(group_width, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n\n        if last_gamma:\n            from torch.nn.init import zeros_\n\n            zeros_(self.bn3.weight)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n        self.ca1 = ChannelAttention1d(group_width)\n        self.sa1 = SpatialAttention1d()\n\n        self.se = SEModule(group_width)\n        \n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        #x = self.ca1(x) * x\n        #位置2，空间注意力\n       \n        out = self.sa1(out) * out\n        out = self.bn1(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock1(out)\n        out = self.relu(out)\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n        #out = self.ca1(out) * out\n        #out=  self.sa1(out) * out\n        #out = self.se(out) * out\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            if self.dropblock_prob > 0.0:\n                out = self.dropblock2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T06:24:35.397816Z","iopub.execute_input":"2024-02-11T06:24:35.398219Z","iopub.status.idle":"2024-02-11T06:24:38.404132Z","shell.execute_reply.started":"2024-02-11T06:24:35.398192Z","shell.execute_reply":"2024-02-11T06:24:38.403328Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class SplAtConv1d_WithoutCA(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1,\n            groups=1,\n            bias=True,\n            radix=2,\n            reduction_factor=4,\n            norm_layer=None,\n            dropblock_prob=0.0,\n            **kwargs,\n\n    ):\n        super().__init__()\n\n        self.dropblock_prob = dropblock_prob\n        inter_channels = max(in_channels * radix // reduction_factor, 32)\n        self.radix = radix\n        self.cardinality = groups\n        self.channels = channels\n\n        self.conv = nn.Conv1d(\n            in_channels,\n            channels * radix,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups=groups * radix,\n            bias=bias,\n            **kwargs,\n        )\n        self.bn0 = norm_layer(self.channels * radix)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc1 = nn.Conv1d(self.channels, inter_channels, 1, groups=self.cardinality)\n        self.bn1 = norm_layer(inter_channels)\n        self.fc2 = nn.Conv1d(inter_channels, self.channels * radix, 1, groups=self.cardinality)\n        if dropblock_prob > 0.0:\n            self.dropblock = DropBlock1D(dropblock_prob, 3)\n        self.rsoftmax = rSoftMax(radix, self.cardinality)\n        self.se = SEModule(in_channels)\n        self.ca1 = ChannelAttention1d(in_channels)\n        self.sa1 = SpatialAttention1d()\n\n    def forward(self, x):\n        #x = self.ca1(x) * x\n        #x=  self.sa1(x) * x\n        #x = self.se(x) * x\n        x = self.conv(x)\n        x = self.bn0(x)\n        if self.dropblock_prob > 0.0:\n            x = self.dropblock(x)\n        x = self.relu(x)\n\n        batch, rchannel = x.shape[:2]\n        if self.radix > 1:\n            splited = torch.split(x, int(rchannel // self.radix), dim=1)\n            gap = sum(splited)\n        else:\n            gap = x\n        #se放在这个位置结果非常好\n        #位置1，通道注意力\n        #gap = self.ca1(gap) * gap\n        #gap = self.sa1(gap) * gap\n        #gap = self.se(gap) * gap\n        gap = F.adaptive_avg_pool1d(gap, 1)\n        gap = self.fc1(gap)\n        gap = self.bn1(gap)\n        gap = self.relu(gap)\n\n        atten = self.fc2(gap)\n        atten = self.rsoftmax(atten).view(batch, -1, 1)\n\n        if self.radix > 1:\n            attens = torch.split(atten, int(rchannel // self.radix), dim=1)\n            outs = []\n            for att, split in zip(attens, splited):\n                outs.append(att * split)\n            out = sum(outs)\n        else:\n            out = atten * x\n       # out = self.se(out) * out\n        #out = self.ca1(out) * out\n        #out = self.sa1(out) * out\n        return out.contiguous()\n    \nclass ResNeStBottleneck_WithoutCA(nn.Module):\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            radix=1,\n            cardinality=1,\n            bottleneck_width=64,\n            avd=False,\n            avd_first=False,\n            dilation=1,\n            is_first=False,\n            norm_layer=None,\n            last_gamma=False,\n            dropblock_prob=0.0\n    ):\n        super().__init__()\n        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n\n        self.conv1 = nn.Conv1d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.radix = radix\n        self.avd = avd and (stride > 1 or is_first)\n        self.avd_first = avd_first\n        self.dropblock_prob = dropblock_prob\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool1d(3, stride, padding=1)\n            stride = 1\n        if dropblock_prob > 0.0:\n            self.dropblock1 = DropBlock1D(dropblock_prob, 3)\n            if radix == 1:\n                self.dropblock2 = DropBlock1D(dropblock_prob, 3)\n            self.dropblock3 = DropBlock1D(dropblock_prob, 3)\n\n        if radix >= 1:\n            self.conv2 = SplAtConv1d_WithoutCA(\n                group_width,\n                group_width,\n                kernel_size=3,\n                stride=stride,\n                padding=dilation,\n                dilation=dilation,\n                groups=cardinality,\n                bias=False,\n                radix=radix,\n                norm_layer=norm_layer,\n                dropblock_prob=dropblock_prob\n            )\n        else:\n            self.conv2 = nn.Conv1d(\n                group_width,\n                group_width,\n                kernel_size=3,\n                stride=stride,\n                padding=dilation,\n                dilation=dilation,\n                groups=cardinality,\n                bias=False,\n            )\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv1d(group_width, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n\n        if last_gamma:\n            from torch.nn.init import zeros_\n\n            zeros_(self.bn3.weight)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n        self.ca1 = ChannelAttention1d(group_width)\n        self.sa1 = SpatialAttention1d()\n\n        self.se = SEModule(group_width)\n        \n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        #x = self.ca1(x) * x\n        #位置2，空间注意力\n        #x=  self.sa1(x) * x\n        out = self.sa1(out) * out\n        out = self.bn1(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock1(out)\n        out = self.relu(out)\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n        #out = self.ca1(out) * out\n        #out=  self.sa1(out) * out\n        #out = self.se(out) * out\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            if self.dropblock_prob > 0.0:\n                out = self.dropblock2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNeStBottleneck_WithoutSA(nn.Module):\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            radix=1,\n            cardinality=1,\n            bottleneck_width=64,\n            avd=False,\n            avd_first=False,\n            dilation=1,\n            is_first=False,\n            norm_layer=None,\n            last_gamma=False,\n            dropblock_prob=0.0\n    ):\n        super().__init__()\n        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n\n        self.conv1 = nn.Conv1d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.radix = radix\n        self.avd = avd and (stride > 1 or is_first)\n        self.avd_first = avd_first\n        self.dropblock_prob = dropblock_prob\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool1d(3, stride, padding=1)\n            stride = 1\n        if dropblock_prob > 0.0:\n            self.dropblock1 = DropBlock1D(dropblock_prob, 3)\n            if radix == 1:\n                self.dropblock2 = DropBlock1D(dropblock_prob, 3)\n            self.dropblock3 = DropBlock1D(dropblock_prob, 3)\n\n        if radix >= 1:\n            self.conv2 = SplAtConv1d(\n                group_width,\n                group_width,\n                kernel_size=3,\n                stride=stride,\n                padding=dilation,\n                dilation=dilation,\n                groups=cardinality,\n                bias=False,\n                radix=radix,\n                norm_layer=norm_layer,\n                dropblock_prob=dropblock_prob\n            )\n        else:\n            self.conv2 = nn.Conv1d(\n                group_width,\n                group_width,\n                kernel_size=3,\n                stride=stride,\n                padding=dilation,\n                dilation=dilation,\n                groups=cardinality,\n                bias=False,\n            )\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv1d(group_width, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n\n        if last_gamma:\n            from torch.nn.init import zeros_\n\n            zeros_(self.bn3.weight)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n        self.ca1 = ChannelAttention1d(group_width)\n        self.sa1 = SpatialAttention1d()\n\n        self.se = SEModule(group_width)\n        \n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        #x = self.ca1(x) * x\n        #位置2，空间注意力\n       \n        #out = self.sa1(out) * out\n        out = self.bn1(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock1(out)\n        out = self.relu(out)\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n        #out = self.ca1(out) * out\n        #out=  self.sa1(out) * out\n        #out = self.se(out) * out\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            if self.dropblock_prob > 0.0:\n                out = self.dropblock2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n    \nclass ResNeStBottleneck_WithoutCASA(nn.Module):\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            radix=1,\n            cardinality=1,\n            bottleneck_width=64,\n            avd=False,\n            avd_first=False,\n            dilation=1,\n            is_first=False,\n            norm_layer=None,\n            last_gamma=False,\n            dropblock_prob=0.0\n    ):\n        super().__init__()\n        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n\n        self.conv1 = nn.Conv1d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.radix = radix\n        self.avd = avd and (stride > 1 or is_first)\n        self.avd_first = avd_first\n        self.dropblock_prob = dropblock_prob\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool1d(3, stride, padding=1)\n            stride = 1\n        if dropblock_prob > 0.0:\n            self.dropblock1 = DropBlock1D(dropblock_prob, 3)\n            if radix == 1:\n                self.dropblock2 = DropBlock1D(dropblock_prob, 3)\n            self.dropblock3 = DropBlock1D(dropblock_prob, 3)\n\n        if radix >= 1:\n            self.conv2 = SplAtConv1d_WithoutCA(\n                group_width,\n                group_width,\n                kernel_size=3,\n                stride=stride,\n                padding=dilation,\n                dilation=dilation,\n                groups=cardinality,\n                bias=False,\n                radix=radix,\n                norm_layer=norm_layer,\n                dropblock_prob=dropblock_prob\n            )\n        else:\n            self.conv2 = nn.Conv1d(\n                group_width,\n                group_width,\n                kernel_size=3,\n                stride=stride,\n                padding=dilation,\n                dilation=dilation,\n                groups=cardinality,\n                bias=False,\n            )\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv1d(group_width, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n\n        if last_gamma:\n            from torch.nn.init import zeros_\n\n            zeros_(self.bn3.weight)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n        self.ca1 = ChannelAttention1d(group_width)\n        self.sa1 = SpatialAttention1d()\n\n        self.se = SEModule(group_width)\n        \n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        #x = self.ca1(x) * x\n        #位置2，空间注意力\n       \n        #out = self.sa1(out) * out\n        out = self.bn1(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock1(out)\n        out = self.relu(out)\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n        #out = self.ca1(out) * out\n        #out=  self.sa1(out) * out\n        #out = self.se(out) * out\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            if self.dropblock_prob > 0.0:\n                out = self.dropblock2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        out += residual\n        out = self.relu(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-02-11T06:24:38.405655Z","iopub.execute_input":"2024-02-11T06:24:38.406341Z","iopub.status.idle":"2024-02-11T06:24:38.463930Z","shell.execute_reply.started":"2024-02-11T06:24:38.406307Z","shell.execute_reply":"2024-02-11T06:24:38.462896Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\n\nclass ResNeSt1d(nn.Module):\n    def __init__(\n            self,\n            inchannels,\n            block,\n            layers,\n            radix=1,\n            groups=1,\n            bottleneck_width=64,\n            num_classes=1000,\n            dilated=False,\n            dilation=1,\n            deep_stem=False,\n            stem_width=64,\n            avg_down=False,\n            avd=False,\n            avd_first=False,\n            final_drop=0.0,\n            last_gamma=False,\n            norm_layer=nn.BatchNorm1d,\n            dropblock_prob=0\n    ):\n        super().__init__()\n\n        self.cardinality = groups\n        self.bottleneck_width = bottleneck_width\n        # ResNet-D params\n        self.inplanes = stem_width * 2 if deep_stem else 64\n        self.avg_down = avg_down\n        self.last_gamma = last_gamma\n        # ResNeSt params\n        self.radix = radix\n        self.avd = avd\n        self.avd_first = avd_first\n        \n        \n        act = nn.ReLU\n\n        if deep_stem:\n            self.conv1 = nn.Sequential(\n                nn.Conv1d(inchannels, stem_width, 3, 2, 1, bias=False),\n                norm_layer(stem_width),\n                act(inplace=True),\n                nn.Conv1d(stem_width, stem_width, 3, 1, 1, bias=False),\n                norm_layer(stem_width),\n                act(inplace=True),\n                nn.Conv1d(stem_width, self.inplanes, 3, 1, 1, bias=False),\n            )\n        else:\n            self.conv1 = nn.Conv1d(inchannels, self.inplanes, 7, 2, 3, bias=False)\n\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = act(inplace=True)\n        self.maxpool = nn.MaxPool1d(3, 2, 1)\n        b = 16\n        Blist = [b * 2, b * 4, b * 8, b * 16]\n        self.layer1 = self._make_layer(block, Blist[0], layers[0], norm_layer=norm_layer, is_first=False)\n        self.layer2 = self._make_layer(block, Blist[1], layers[1], stride=2, norm_layer=norm_layer)\n        if dilated or dilation == 4:\n            self.layer3 = self._make_layer(block, Blist[2], layers[2], stride=1, dilation=2, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            # self.layer4 = self._make_layer(block, Blist[3], layers[2], stride=1, dilation=2, norm_layer=norm_layer,dropblock_prob=dropblock_prob)\n        elif dilation == 2:\n            self.layer3 = self._make_layer(block, Blist[2], layers[2], stride=2, dilation=1, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            # self.layer4 = self._make_layer(block, Blist[3], layers[2], stride=1, dilation=2, norm_layer=norm_layer,dropblock_prob=dropblock_prob)\n        else:\n            self.layer3 = self._make_layer(block, Blist[2], layers[2], stride=2, dilation=1, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            # self.layer4 = self._make_layer(block, Blist[3], layers[2], stride=2, dilation=1, norm_layer=norm_layer,dropblock_prob=dropblock_prob)\n\n        self.avgpool = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten())\n        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n        self.fc = nn.Linear(Blist[2] * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None, is_first=True,\n                    dropblock_prob=0.0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            down_layers = []\n            if self.avg_down:\n                if dilation == 1:\n                    down_layers.append(nn.AvgPool1d(stride, stride, ceil_mode=True, count_include_pad=False))\n                else:\n                    down_layers.append(nn.AvgPool1d(1, 1, ceil_mode=True, count_include_pad=False))\n                down_layers.append(nn.Conv1d(self.inplanes, planes * block.expansion, 1, 1, 0, bias=False))\n            else:\n                down_layers.append(nn.Conv1d(self.inplanes, planes * block.expansion, 1, stride, 0, bias=False))\n\n            down_layers.append(norm_layer(planes * block.expansion))\n            downsample = nn.Sequential(*down_layers)\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    stride,\n                    downsample=downsample,\n                    radix=self.radix,\n                    cardinality=self.cardinality,\n                    bottleneck_width=self.bottleneck_width,\n                    avd=self.avd,\n                    avd_first=self.avd_first,\n                    dilation=1,\n                    is_first=is_first,\n                    norm_layer=norm_layer,\n                    last_gamma=self.last_gamma,\n                    dropblock_prob=dropblock_prob\n                )\n            )\n        elif dilation == 4:\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    stride,\n                    downsample=downsample,\n                    radix=self.radix,\n                    cardinality=self.cardinality,\n                    bottleneck_width=self.bottleneck_width,\n                    avd=self.avd,\n                    avd_first=self.avd_first,\n                    dilation=2,\n                    is_first=is_first,\n                    norm_layer=norm_layer,\n                    last_gamma=self.last_gamma,\n                    dropblock_prob=dropblock_prob\n                )\n            )\n        else:\n            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    radix=self.radix,\n                    cardinality=self.cardinality,\n                    bottleneck_width=self.bottleneck_width,\n                    avd=self.avd,\n                    avd_first=self.avd_first,\n                    dilation=dilation,\n                    norm_layer=norm_layer,\n                    last_gamma=self.last_gamma,\n                    dropblock_prob=dropblock_prob\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = x.permute(0, -1, 1)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        # x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        \n        if self.drop:\n            x = self.drop(x)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnest_t(inchannels,block, **kwargs):\n    model = ResNeSt1d(\n        inchannels,\n        block,\n        [1, 1, 2, 2],\n        radix=4,\n        groups=4,\n        bottleneck_width=8,\n        deep_stem=True,\n        #wisdm = 16, pamp2 = 32\n        stem_width=32,\n        avg_down=True,\n        avd=True,\n        avd_first=False,\n        **kwargs,\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-11T06:24:38.466930Z","iopub.execute_input":"2024-02-11T06:24:38.467353Z","iopub.status.idle":"2024-02-11T06:24:38.497022Z","shell.execute_reply.started":"2024-02-11T06:24:38.467319Z","shell.execute_reply":"2024-02-11T06:24:38.496036Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import copy\nimport time\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport pandas as pd\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, x_path, y_path, transform=None):\n        self.x_data = np.load(x_path)\n        self.y_data = np.load(y_path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.x_data)\n\n    def __getitem__(self, idx):\n        x = self.x_data[idx]\n        y = self.y_data[idx]\n\n        if self.transform:\n            x = self.transform(x)\n\n        return x, y\n#删除1\nclass LabelSmoothingCrossEntropy(nn.Module):\n    \"\"\" NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.1):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        assert smoothing < 1.0\n        self.smoothing = smoothing\n        self.confidence = 1. - smoothing\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        logprobs = F.log_softmax(input, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1).to(torch.int64))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\nclass ClassBalancedLoss(nn.Module):\n\n    def __init__(self, num_classes, beta):\n        super().__init__()\n        self.num_classes = num_classes\n        self.beta = beta\n\n    def forward(self, inputs, targets):\n        inputs = inputs.float()\n        targets = targets.long()\n        loss = F.cross_entropy(inputs, targets)\n\n        return self.beta * loss\n\n#删除2\nclass FocalLoss(nn.Module):\n\n    def __init__(self, alpha=0.25, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        inputs = inputs.float()\n        targets = targets.long()\n\n        ce_loss = F.cross_entropy(inputs, targets)\n        pt = torch.exp(-ce_loss)\n\n        focal_loss = ((1-pt)**self.gamma * ce_loss)\n\n        return focal_loss\n#删除3\nclass MultiLoss(nn.Module):\n    def __init__(self, num_classes, beta, weights, smoothing=0.15):\n        super().__init__()\n\n        # Replace ClassBalancedLoss with LabelSmoothingLoss\n        self.label_smoothing_loss = LabelSmoothingCrossEntropy(smoothing=0.1)\n        self.focal_loss = FocalLoss()\n        self.ce_loss=ClassBalancedLoss(num_classes,beta)\n        self.weights = weights\n\n\n    def forward(self, inputs, targets):\n        loss1 = self.weights[0] * self.label_smoothing_loss(inputs, targets)\n        loss2 = self.weights[1] * self.focal_loss(inputs, targets)\n        loss3 = self.weights[2] * self.ce_loss(inputs, targets)\n        return loss1 + loss2 + loss3\n    #删除4\n    def update_weights(self, epoch, performance_metrics):\n        # 根据验证准确率调整权重\n        task1_acc = performance_metrics.get(\"task1\", 0.0)\n        task2_acc = performance_metrics.get(\"task2\", 0.0)\n        task3_acc = performance_metrics.get(\"task3\", 0.0)\n\n        # 根据验证准确率的倒数来调整权重\n        # 准确率越高的任务将获得较小的权重，反之亦然\n        self.weights[0] = 0.5*(1-self.weights[1])\n        self.weights[1] = 1-(1.0 / (task1_acc + 1e-8)-1)-a\n        self.weights[2] = 0.5*(1-self.weights[1])\n\n        print(f\"Epoch {epoch}: 更新权重 - 任务1: {self.weights[0]}, 任务2: {self.weights[1]},任务3: {self.weights[2]}\")\n\nclass MultiLoss_Withoutweight(nn.Module):\n    def __init__(self, num_classes, beta, weights, smoothing=0.15):\n        super().__init__()\n\n        # Replace ClassBalancedLoss with LabelSmoothingLoss\n        self.label_smoothing_loss = LabelSmoothingCrossEntropy(smoothing=0.1)\n        self.focal_loss = FocalLoss()\n        self.ce_loss=ClassBalancedLoss(num_classes,beta)\n        self.weights = weights\n\n    def forward(self, inputs, targets):\n        loss1 = self.label_smoothing_loss(inputs, targets)\n        loss2 = self.focal_loss(inputs, targets)\n        loss3 = self.ce_loss(inputs, targets)\n        return loss1 + loss2 + loss3\n\n        \n        \n #删除5       \ndef compute_validation_accuracy(model, val_dataloader, device):\n    model.eval()\n    corrects = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            corrects += (predicted == labels).sum().item()\n\n    accuracy = corrects / total\n    return accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T06:24:38.498599Z","iopub.execute_input":"2024-02-11T06:24:38.498974Z","iopub.status.idle":"2024-02-11T06:24:38.525188Z","shell.execute_reply.started":"2024-02-11T06:24:38.498943Z","shell.execute_reply":"2024-02-11T06:24:38.524130Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\ndef train_val_data_process():\n    # 训练数据集的路径\n    x_train_path = \"/kaggle/input/wisdm-data/wisdm/x_train.npy\"\n    y_train_path = \"/kaggle/input/wisdm-data/wisdm/y_train.npy\"\n\n    train_dataset = CustomDataset(x_train_path, y_train_path)\n\n    # 将数据集拆分为训练集和验证集\n    train_size = int(0.8 * len(train_dataset))\n    val_size = len(train_dataset) - train_size\n    train_data, val_data = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n\n    # 定义训练集和验证集的数据加载器\n\n    train_dataloader = DataLoader(train_data, batch_size=512, shuffle=True, num_workers=2)#wisdm  batch_size=512  pamp2  batch_size=256\n    val_dataloader = DataLoader(val_data, batch_size=512, shuffle=True, num_workers=2)#wisdm  batch_size=512  pamp2  batch_size=256\n\n    return train_dataloader, val_dataloader\n\n\ndef train_model_process(model, train_dataloader, val_dataloader, num_epochs,criterion):\n    # 设定训练所用到的设备，有GPU用GPU没有GPU用CPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # 使用Adam优化器，学习率为0.001\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)#wisdm0.005  pamap2  0.01\n\n    # 将模型放入到训练设备中\n    model = model.to(device)\n    # 复制当前模型的参数\n    best_model_wts = copy.deepcopy(model.state_dict())\n\n    # 初始化参数\n    # 最高准确度\n    best_acc = 0.0\n    # 训练集损失列表\n    train_loss_all = []\n    # 验证集损失列表\n    val_loss_all = []\n    # 训练集准确度列表\n    train_acc_all = []\n    # 验证集准确度列表\n    val_acc_all = []\n    # 当前时间\n    since = time.time()\n    #删除6\n    multi_loss = MultiLoss(num_classes, beta, weights)\n\n\n    for epoch in range(num_epochs):\n        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n        print(\"-\" * 10)\n\n        # 初始化参数\n        # 训练集损失函数\n        train_loss = 0.0\n        # 训练集准确度\n        train_corrects = 0\n        # 验证集损失函数\n        val_loss = 0.0\n        # 验证集准确度\n        val_corrects = 0\n        # 训练集样本数量\n        train_num = 0\n        # 验证集样本数量\n        val_num = 0\n\n        # 对每一个mini-batch训练和计算\n        for step, (b_x, b_y) in enumerate(train_dataloader):\n            # 将特征放入到训练设备中\n            b_x = b_x.to(device)\n            # 将标签放入到训练设备中\n            b_y = b_y.to(device)\n            # 设置模型为训练模式\n            model.train()\n\n            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n            output = model(b_x)\n            # 查找每一行中最大值对应的行标\n            pre_lab = torch.argmax(output, dim=1)\n            # 计算每一个batch的损失函数\n            loss = criterion(output, b_y)\n\n            # 将梯度初始化为0\n            optimizer.zero_grad()\n            # 反向传播计算\n            loss.backward()\n            # 根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值的作用\n            optimizer.step()\n            # 对损失函数进行累加\n            train_loss += loss.item() * b_x.size(0)\n            # 如果预测正确，则准确度train_corrects加1\n            train_corrects += torch.sum(pre_lab == b_y.data)\n            # 当前用于训练的样本数量\n            train_num += b_x.size(0)\n        for step, (b_x, b_y) in enumerate(val_dataloader):\n            # 将特征放入到验证设备中\n            b_x = b_x.to(device)\n            # 将标签放入到验证设备中\n            b_y = b_y.to(device)\n            # 设置模型为评估模式\n            model.eval()\n            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n            output = model(b_x)\n            # 查找每一行中最大值对应的行标\n            pre_lab = torch.argmax(output, dim=1)\n            # 计算每一个batch的损失函数\n            loss = criterion(output, b_y)\n\n            # 对损失函数进行累加\n            val_loss += loss.item() * b_x.size(0)\n            # 如果预测正确，则准确度train_corrects加1\n            val_corrects += torch.sum(pre_lab == b_y.data)\n            # 当前用于验证的样本数量\n            val_num += b_x.size(0)\n\n        # 计算并保存每一次迭代的loss值和准确率\n        # 计算并保存训练集的loss值\n        train_loss_all.append(train_loss / train_num)\n        # 计算并保存训练集的准确率\n        train_acc_all.append(train_corrects.double().item() / train_num)\n\n        # 计算并保存验证集的loss值\n        val_loss_all.append(val_loss / val_num)\n        # 计算并保存验证集的准确率\n        val_acc_all.append(val_corrects.double().item() / val_num)\n\n        print(\"{} train loss:{:.4f} train acc: {:.4f}\".format(epoch, train_loss_all[-1], train_acc_all[-1]))\n        print(\"{} val loss:{:.4f} val acc: {:.4f}\".format(epoch, val_loss_all[-1], val_acc_all[-1]))\n\n        if val_acc_all[-1] > best_acc:\n            # 保存当前最高准确度\n            best_acc = val_acc_all[-1]\n            # 保存当前最高准确度的模型参数\n            best_model_wts = copy.deepcopy(model.state_dict())\n\n        # 计算训练和验证的耗时\n        time_use = time.time() - since\n        print(\"训练和验证耗费的时间{:.0f}m{:.0f}s\".format(time_use // 60, time_use % 60))\n        #删除7\n        val_acc_task1 = compute_validation_accuracy(model, val_dataloader, device)\n        val_acc_task2 = compute_validation_accuracy(model, val_dataloader, device)\n        val_acc_task3 = compute_validation_accuracy(model, val_dataloader, device)\n        multi_loss.update_weights(epoch, {\"task1\": val_acc_task1, \"task2\": val_acc_task2, \"task3\": val_acc_task3})\n        multi_loss = MultiLoss(num_classes, beta, multi_loss.weights)\n\n\n    # 选择最优参数，保存最优参数的模型\n    model.load_state_dict(best_model_wts)\n    torch.save(best_model_wts, \"/kaggle/working/best_model.pth\")\n    print(best_acc)\n    train_process = pd.DataFrame(data={\"epoch\": range(num_epochs),\n                                       \"train_loss_all\": train_loss_all,\n                                       \"val_loss_all\": val_loss_all,\n                                       \"train_acc_all\": train_acc_all,\n                                       \"val_acc_all\": val_acc_all, })\n\n    return train_process\n\n\ndef matplot_acc_loss(train_process):\n    # 显示每一次迭代后的训练集和验证集的损失函数和准确率\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(train_process['epoch'], train_process.train_loss_all, \"ro-\", label=\"Train loss\")\n    plt.plot(train_process['epoch'], train_process.val_loss_all, \"bs-\", label=\"Val loss\")\n    plt.legend()\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"Loss\")\n    plt.subplot(1, 2, 2)\n    plt.plot(train_process['epoch'], train_process.train_acc_all, \"ro-\", label=\"Train acc\")\n    plt.plot(train_process['epoch'], train_process.val_acc_all, \"bs-\", label=\"Val acc\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"acc\")\n    plt.legend()\n    plt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T06:24:38.526656Z","iopub.execute_input":"2024-02-11T06:24:38.526947Z","iopub.status.idle":"2024-02-11T06:24:38.553445Z","shell.execute_reply.started":"2024-02-11T06:24:38.526912Z","shell.execute_reply":"2024-02-11T06:24:38.552492Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# search pamp to find the differences and change\nnum_classes = 6 #pamp2 =12 wisdm =6\ninchannels = 3 #pamp2 =36 wisdm =3\nmodel_n_casa = resnest_t(inchannels=inchannels,num_classes=num_classes,block = ResNeStBottleneck_WithoutCASA)\nmodel_n_ca = resnest_t(inchannels=inchannels,num_classes=num_classes,block = ResNeStBottleneck_WithoutCA)\nmodel_n_sa = resnest_t(inchannels=inchannels,num_classes=num_classes,block = ResNeStBottleneck_WithoutSA)\nmodel = resnest_t(inchannels=inchannels,num_classes=num_classes,block = ResNeStBottleneck)\nmodel = model\nif __name__ == '__main__':\n    # 加载需要的模型\n    \n    # 加载数据集\n    train_data, val_data = train_val_data_process()\n    # 利用现有的模型进行模型的训练\n      # 替换为你的类别总数\n    beta = 0.8  # 平衡系数  #wisdm\n    #beta=0.8  #pamp2\n    weights = [0.1, 0.8,0.1] #wisdm\n    #weights = [0.25, 0.5,0.25] #pamp2\n    #a=0.45\n    a=0.17\n    criterion1 = nn.CrossEntropyLoss()\n    criterion2 = MultiLoss_Withoutweight(num_classes, beta, weights)\n    criterion3 = MultiLoss(num_classes, beta, weights,smoothing=0.15)#smoothing=0.15  #pamp2\n\n    train_process = train_model_process(model, train_data, val_data, num_epochs=40,criterion = criterion2)\n    matplot_acc_loss(train_process)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T06:24:38.554555Z","iopub.execute_input":"2024-02-11T06:24:38.554843Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 0/39\n----------\n0 train loss:1.3063 train acc: 0.8362\n0 val loss:1.0234 val acc: 0.8871\n训练和验证耗费的时间0m4s\nEpoch 0: 更新权重 - 任务1: 0.09999999999999998, 任务2: 0.7027062875699671,任务3: 0.14864685621501644\nEpoch 1/39\n----------\n1 train loss:0.7508 train acc: 0.9407\n1 val loss:0.8962 val acc: 0.9003\n训练和验证耗费的时间0m8s\nEpoch 1: 更新权重 - 任务1: 0.14864685621501644, 任务2: 0.7192401185459926,任务3: 0.14037994072700372\nEpoch 2/39\n----------\n2 train loss:0.6721 train acc: 0.9577\n2 val loss:0.7117 val acc: 0.9484\n训练和验证耗费的时间0m12s\nEpoch 2: 更新权重 - 任务1: 0.14037994072700372, 任务2: 0.7756313121250947,任务3: 0.11218434393745264\nEpoch 3/39\n----------\n3 train loss:0.6256 train acc: 0.9667\n3 val loss:0.6423 val acc: 0.9657\n训练和验证耗费的时间0m16s\nEpoch 3: 更新权重 - 任务1: 0.11218434393745264, 任务2: 0.7945214628673726,任务3: 0.10273926856631371\nEpoch 4/39\n----------\n4 train loss:0.6069 train acc: 0.9706\n4 val loss:0.7079 val acc: 0.9475\n训练和验证耗费的时间0m20s\nEpoch 4: 更新权重 - 任务1: 0.10273926856631371, 任务2: 0.7746179832670964,任务3: 0.1126910083664518\nEpoch 5/39\n----------\n5 train loss:0.5910 train acc: 0.9754\n5 val loss:0.6343 val acc: 0.9647\n训练和验证耗费的时间0m24s\nEpoch 5: 更新权重 - 任务1: 0.1126910083664518, 任务2: 0.7934218396534974,任务3: 0.10328908017325128\nEpoch 6/39\n----------\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport numpy as np\nimport torch.nn as nn\n\nclass CustomDataset(Dataset):\n    def __init__(self, x_path, y_path, transform=None):\n        self.x_data = np.load(x_path)\n        self.y_data = np.load(y_path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.x_data)\n\n    def __getitem__(self, idx):\n        x = self.x_data[idx]\n        y = self.y_data[idx]\n\n        if self.transform:\n            x = self.transform(x)\n\n        return x, y\n\ndef test_final(model, test_dataloader):\n    device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n    model = model.to(device)\n    criterion1 = nn.CrossEntropyLoss()\n    num_classes = 6\n    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)  \n\n    model.eval()\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    y_true = []\n    y_pred = []\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(test_dataloader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = criterion1(outputs, labels)\n            _, predicted = torch.max(outputs.data, 1)\n            test_correct += (predicted == labels).sum().item()\n            test_total += labels.size(0)\n            test_loss += loss.item()\n            y_true.extend(labels.tolist())\n            y_pred.extend(predicted.tolist())\n            conf_matrix += confusion_matrix(labels.cpu(), predicted.cpu(), labels=range(num_classes))\n\n    report = classification_report(y_true, y_pred,digits=4)\n    test_acc = 100.0 * test_correct / test_total\n    test_loss = test_loss / len(test_dataloader)\n\n    g_mean = np.sqrt(np.diag(conf_matrix) / np.sum(conf_matrix, axis=1))\n    g_mean = np.mean(g_mean)\n    report += '\\nG-mean: {:.4f}'.format(g_mean)\n    print('Test Loss: {:.4f}, Test Acc: {:.2f}%, G-mean: {:.4f}'.format(test_loss, test_acc, g_mean))\n    print(report)\n\n\ndef test_data_process():\n    x_test_path = \"/kaggle/input/wisdm-data/wisdm/x_test.npy\"\n    y_test_path = \"/kaggle/input/wisdm-data/wisdm/y_test.npy\"\n    test_data = CustomDataset(x_test_path, y_test_path)\n    test_dataloader = DataLoader(test_data, batch_size=512, shuffle=True, num_workers=2)\n    return test_dataloader\n\n\ndef test_model_process(model, test_dataloader):\n    device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n    model = model.to(device)\n    classes = ['Downstairs', 'Jogging', 'Sitting', 'Standing', 'Upstairs', 'Walking']\n\n    test_corrects = 0.0\n    test_num = 0\n\n    with torch.no_grad():\n        for test_data_x, test_data_y in test_dataloader:\n            test_data_x = test_data_x.to(device)\n            test_data_y = test_data_y.to(device)\n            model.eval()\n            output = model(test_data_x)\n            pre_lab = torch.argmax(output, dim=1)\n            test_corrects += torch.sum(pre_lab == test_data_y.data)\n            test_num += test_data_x.size(0)\n\n    test_acc = test_corrects.double().item() / test_num\n    print(\"测试的准确率为：\", test_acc)\n\n\n\nif __name__ == \"__main__\":\n    model.load_state_dict(torch.load('best_model.pth'))\n    test_dataloader = test_data_process()\n    test_final(model, test_dataloader)\n  \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}